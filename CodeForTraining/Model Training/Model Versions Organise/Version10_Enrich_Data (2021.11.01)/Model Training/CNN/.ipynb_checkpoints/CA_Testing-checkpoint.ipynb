{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5soa5HwN7iA7",
    "outputId": "06d56375-4306-477c-c935-666536a18584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5_xNqcc7nxs",
    "outputId": "bb116026-331c-4edc-baf4-c59626922b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36kITWPc7n7x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7ovg1fa7oFj"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "FusdprF07oPa",
    "outputId": "946aabf0-ae72-45f3-e989-cec39e703e7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>world so cold</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>It starts with pain, followed by hate\\nFueled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>broken</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Freedom!\\nAlone again again alone\\nPatiently w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>3 leaf loser</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Biting the hand that feeds you, lying to the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>anthem for the underdog</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>You say you know just who I am\\nBut you can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>adrenaline</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>My heart is beating faster can't control these...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290178</th>\n",
       "      <td>bobby womack</td>\n",
       "      <td>i wish he didn t trust me so much</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>en</td>\n",
       "      <td>I'm the best friend he's got I'd give him the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290179</th>\n",
       "      <td>bad boys blue</td>\n",
       "      <td>i totally miss you</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Bad Boys Blue \"I Totally Miss You\" I did you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290180</th>\n",
       "      <td>celine dion</td>\n",
       "      <td>sorry for love</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Forgive me for the things That I never said to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290181</th>\n",
       "      <td>dan bern</td>\n",
       "      <td>cure for aids</td>\n",
       "      <td>Indie</td>\n",
       "      <td>en</td>\n",
       "      <td>The day they found a cure for AIDS The day the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290182</th>\n",
       "      <td>crawdad republic</td>\n",
       "      <td>iceberg meadows</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Fourth of July has come, it's custom that we g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290183 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Artist  ...                                             Lyrics\n",
       "0              12 stones  ...  It starts with pain, followed by hate\\nFueled ...\n",
       "1              12 stones  ...  Freedom!\\nAlone again again alone\\nPatiently w...\n",
       "2              12 stones  ...  Biting the hand that feeds you, lying to the v...\n",
       "3              12 stones  ...  You say you know just who I am\\nBut you can't ...\n",
       "4              12 stones  ...  My heart is beating faster can't control these...\n",
       "...                  ...  ...                                                ...\n",
       "290178      bobby womack  ...  I'm the best friend he's got I'd give him the ...\n",
       "290179     bad boys blue  ...  Bad Boys Blue \"I Totally Miss You\" I did you w...\n",
       "290180       celine dion  ...  Forgive me for the things That I never said to...\n",
       "290181          dan bern  ...  The day they found a cure for AIDS The day the...\n",
       "290182  crawdad republic  ...  Fourth of July has come, it's custom that we g...\n",
       "\n",
       "[290183 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "WECpbTOuol1N",
    "outputId": "54630ec6-2f9d-4982-b52d-ca0da66a0df0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Song year</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>craftsmanship</td>\n",
       "      <td>2005</td>\n",
       "      <td>buck-65</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Most folks spend their days daydreaming of fin...</td>\n",
       "      <td>8294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>come-on-out</td>\n",
       "      <td>2012</td>\n",
       "      <td>the-elwins</td>\n",
       "      <td>Indie</td>\n",
       "      <td>Take your cold hands and put them on my face\\n...</td>\n",
       "      <td>21621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>riot</td>\n",
       "      <td>2013</td>\n",
       "      <td>bullet-for-my-valentine</td>\n",
       "      <td>Metal</td>\n",
       "      <td>Are you ready it's time for war\\nWe'll break d...</td>\n",
       "      <td>3301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that-s-what-girls-do</td>\n",
       "      <td>2007</td>\n",
       "      <td>dream-street</td>\n",
       "      <td>Pop</td>\n",
       "      <td>You ask me why I change the color of my hair\\n...</td>\n",
       "      <td>2773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe-in-a-dollar</td>\n",
       "      <td>2012</td>\n",
       "      <td>cassidy</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Do you believe in magic in a young girl's hear...</td>\n",
       "      <td>16797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>too-little-too-late</td>\n",
       "      <td>2006</td>\n",
       "      <td>amanda-marshall</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Tuesday night - 7:30\\nI hear a voice on the te...</td>\n",
       "      <td>23453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>berserker</td>\n",
       "      <td>2007</td>\n",
       "      <td>aurora-borealis</td>\n",
       "      <td>Metal</td>\n",
       "      <td>Elite forces cloaked in fur un sensitive to pa...</td>\n",
       "      <td>2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>natural-born-killaz</td>\n",
       "      <td>2010</td>\n",
       "      <td>dr-dre</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Dr. Dre]\\nJourney with me\\nInto the mind of a...</td>\n",
       "      <td>24147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>wide-awake</td>\n",
       "      <td>2011</td>\n",
       "      <td>chris-cornell</td>\n",
       "      <td>Rock</td>\n",
       "      <td>You can a look a hurricane right in the eye.\\n...</td>\n",
       "      <td>4150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>god-knows</td>\n",
       "      <td>2007</td>\n",
       "      <td>the-coral</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Deal not the truth till you find the proof\\nAn...</td>\n",
       "      <td>16992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7935 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Song  ...  Track_id\n",
       "0            craftsmanship  ...      8294\n",
       "1              come-on-out  ...     21621\n",
       "2                     riot  ...      3301\n",
       "3     that-s-what-girls-do  ...      2773\n",
       "4      believe-in-a-dollar  ...     16797\n",
       "...                    ...  ...       ...\n",
       "7930   too-little-too-late  ...     23453\n",
       "7931             berserker  ...      2724\n",
       "7932   natural-born-killaz  ...     24147\n",
       "7933            wide-awake  ...      4150\n",
       "7934             god-knows  ...     16992\n",
       "\n",
       "[7935 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6xUxthSkwSs"
   },
   "source": [
    "# Part 1 -- Genre Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiUy31ESk2t3"
   },
   "source": [
    "## Best Model Using embedding training on-the-fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKfUNiSqn61T"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sljDHnhqoB2E"
   },
   "outputs": [],
   "source": [
    "en_train = train[train['Language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCB6COL17om9",
    "outputId": "d3ce28cf-518f-433a-d051-1ebb5341c899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "remove_punc = re.compile('[^0-9a-z ]') # removing any symbol that is NOT number, letter, space.\n",
    "STOPWORDS = set(stopwords.words('english')) # only dealing with English Lyrics, so set stopwords to 'english'\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = text.lower() # lowercase text\n",
    "    text = remove_punc.sub(' ', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with space. \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text\n",
    "\n",
    "en_train['Lyrics'] = en_train['Lyrics'].apply(clean_text)\n",
    "en_train['Lyrics'] = en_train['Lyrics'].str.replace('\\d+', '') # replacing one or more digits by nothing\n",
    "test['Lyrics'] = test['Lyrics'].apply(clean_text)\n",
    "test['Lyrics'] = test['Lyrics'].str.replace('\\d+', '') # replacing one or more digits by nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2OUWXaOtl0G",
    "outputId": "ddd758a7-608d-45ca-b309-89d2e5e217c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         starts pain followed hate fueled endless quest...\n",
       "1         freedom alone alone patiently waiting phone ho...\n",
       "2         biting hand feeds lying voice inside reach beg...\n",
       "3         say know imagine waits across line thought sti...\n",
       "4         heart beating faster control feelings anymore ...\n",
       "                                ...                        \n",
       "290178    best friend got give shirt back knows trust li...\n",
       "290179    bad boys blue totally miss wrong foolish heart...\n",
       "290180    forgive things never said forgive knowing righ...\n",
       "290181    day found cure aids day found cure aids everyb...\n",
       "290182    fourth july come custom go make way  place cal...\n",
       "Name: Lyrics, Length: 250197, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train['Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCoD5dTotnF_",
    "outputId": "74da2798-eb47-424b-f325-13e16c241a48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       folks spend days daydreaming finding clues who...\n",
       "1       take cold hands put face sharpen axe criminal ...\n",
       "2       ready time war break fucking doors smash windo...\n",
       "3       ask change color hair yeah ask need thirty two...\n",
       "4       believe magic young girl heart music free when...\n",
       "                              ...                        \n",
       "7930    tuesday night   hear voice telephone doin fine...\n",
       "7931    elite forces cloaked fur un sensitive pain bur...\n",
       "7932    dr dre journey mind maniac doomed killer since...\n",
       "7933    look hurricane right eye  people dead left die...\n",
       "7934    deal truth till find proof feel weight step to...\n",
       "Name: Lyrics, Length: 7935, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wl2urSeHYcME",
    "outputId": "32eeb090-1f47-49f4-835c-d8d9a6e2612a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:4582: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "en_train['Lyrics'].replace('',np.nan,inplace=True)\n",
    "en_train.dropna(subset=['Lyrics'],inplace=True)\n",
    "\n",
    "lyrics = en_train['Lyrics'].values\n",
    "genre = en_train['Genre'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lyrics_train, lyrics_val, genre_train, genre_val = train_test_split(lyrics, genre, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cY_u1MYyrmxH"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(genre_train)\n",
    "y_train = encoder.transform(genre_train)\n",
    "y_val = encoder.transform(genre_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ygn0MUsyq0-K",
    "outputId": "a812ba26-aa53-4ced-fb05-0309e1b4cc22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  810]\n",
      " [   1  660]\n",
      " [   2  495]\n",
      " [   3  960]\n",
      " [   4  510]\n",
      " [   5  660]\n",
      " [   6  810]\n",
      " [   7 1110]\n",
      " [   8  510]\n",
      " [   9 1410]]\n"
     ]
    }
   ],
   "source": [
    "lyrics_test = test['Lyrics'].values\n",
    "genre_test = test['Genre'].values\n",
    "\n",
    "y_test = encoder.transform(genre_test)\n",
    "\n",
    "(unique, counts) = np.unique(y_test, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjJkyYNyq0-L",
    "outputId": "f4bbd1e0-5564-4528-ea91-e65f9a65be0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  1521]\n",
      " [    1  1615]\n",
      " [    2  6525]\n",
      " [    3  1819]\n",
      " [    4  5781]\n",
      " [    5 10702]\n",
      " [    6 15256]\n",
      " [    7 68956]\n",
      " [    8  2207]\n",
      " [    9 85774]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVKrB0LEq0-L",
    "outputId": "f2117627-5c62-4734-db27-921c86823b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   369]\n",
      " [    1   390]\n",
      " [    2  1644]\n",
      " [    3   419]\n",
      " [    4  1459]\n",
      " [    5  2612]\n",
      " [    6  3877]\n",
      " [    7 17341]\n",
      " [    8   557]\n",
      " [    9 21371]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_val, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3UazR5-q0-L",
    "outputId": "a6f445db-3836-438c-faf1-b3149eeccbb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200156, 10), (50039, 10), (7935, 10))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "dummy_y_train = np_utils.to_categorical(y_train)\n",
    "dummy_y_val = np_utils.to_categorical(y_val)\n",
    "dummy_y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "dummy_y_train.shape, dummy_y_val.shape, dummy_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oB8F-k2q0-M"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer = Tokenizer(num_words=5000)   # num_words is the size of the vocabulary (top 5000 frequent words in the vocabulary)\n",
    "tokenizer.fit_on_texts(lyrics_train)    # Updates internal vocabulary based on a list of texts\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(lyrics_train)  # Transforms each text in texts to a sequence of integers with its corresponding integer value from the word_index dictionary\n",
    "X_val = tokenizer.texts_to_sequences(lyrics_val)\n",
    "X_test = tokenizer.texts_to_sequences(lyrics_test)\n",
    "\n",
    "# The number of unique words in the whole training text\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index for padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vW1anHSjq0-M",
    "outputId": "3bc50ec3-885a-4553-86c8-53ba4ca4151e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4999, 1101, 455, 455, 6, 362, 930, 5, 5, 42, 1604, 516, 8, 46, 7, 12, 42, 1131, 72, 1188, 175, 227, 529, 198, 263, 27, 1130, 2976, 2976, 66, 425, 2562, 2562, 12, 89, 1059, 62, 484, 98, 87, 583, 642, 7, 852, 30, 27, 22, 8, 42, 2312, 10, 295, 474, 294, 278, 5, 19, 211, 1101, 1101, 1101, 1101, 1101, 22, 8, 42, 2312, 10, 295, 474, 294, 278, 5, 19, 211, 2976, 2976, 66, 425, 2562, 2562, 12, 89, 1059, 62, 484, 34], 1473)\n"
     ]
    }
   ],
   "source": [
    "def FindMaxLength(lst):\n",
    "    maxList = max((x) for x in lst)\n",
    "    maxLength = max(len(x) for x in lst )\n",
    "  \n",
    "    return maxList, maxLength\n",
    "\n",
    "print(FindMaxLength(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb0O17Daq0-M",
    "outputId": "c19783c6-9dcf-4da2-965e-b9cac763c14c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200156, 500), (50039, 500), (7935, 500))"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DqhBMc0nwQt"
   },
   "source": [
    "### Testing the best model\n",
    "Best Model is CNN model with 4 epoch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "316AoENClBrE",
    "outputId": "cac6f003-e982-46c4-8532-4b2818a6a8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1-9LYc6KNeX5Yehjk2I4vURPLm6Z_2hoK into ./model/CNN_model_epoch4.h5... Done.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 128)          19328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 297,138\n",
      "Trainable params: 297,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1-9LYc6KNeX5Yehjk2I4vURPLm6Z_2hoK', dest_path='./model/CNN_model_epoch4.h5')\n",
    "\n",
    "best_model_otf = tf.keras.models.load_model('./model/CNN_model_epoch4.h5')\n",
    "\n",
    "best_model_otf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyfKSi36seH5",
    "outputId": "2d41e863-ed1e-4f15-ed7e-20dba75bbbc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Accuracy:  0.7406\n",
      "Validation Dataset Accuracy:  0.6340\n",
      "Testing Dataset Accuracy:  0.2891\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model_otf.evaluate(X_train, dummy_y_train, verbose=False)\n",
    "print(\"Training Dataset Accuracy:  {:.4f}\".format(accuracy))\n",
    "loss, accuracy = best_model_otf.evaluate(X_val, dummy_y_val, verbose=False)\n",
    "print(\"Validation Dataset Accuracy:  {:.4f}\".format(accuracy))\n",
    "loss, accuracy = best_model_otf.evaluate(X_test, dummy_y_test, verbose=False)\n",
    "print(\"Testing Dataset Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdRPD0kBwRuP"
   },
   "source": [
    "Check the result on the Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwNwyj8rwGLm",
    "outputId": "85d64af7-fce5-44ed-f1ee-6c1dbedbfb6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.6733807e-04 7.9748500e-03 4.5280932e-03 ... 8.0110095e-02\n",
      "  1.0576328e-03 3.9466736e-01]\n",
      " [6.9918839e-04 1.1600439e-02 5.7035838e-03 ... 3.1346372e-01\n",
      "  2.0219320e-03 4.8771316e-01]\n",
      " [2.4672030e-02 2.5104450e-03 9.1942489e-02 ... 9.6099094e-02\n",
      "  4.1738544e-03 7.2039658e-01]\n",
      " ...\n",
      " [6.2201274e-05 3.3156143e-03 2.7090317e-04 ... 9.6435934e-01\n",
      "  1.0086500e-02 4.9855388e-03]\n",
      " [1.5521170e-02 3.0508093e-03 1.5095961e-01 ... 2.2138397e-01\n",
      "  5.2005504e-03 5.1328796e-01]\n",
      " [4.7118618e-05 6.8530124e-03 1.9875031e-03 ... 4.2481359e-02\n",
      "  1.1549167e-03 2.7756462e-02]]\n",
      "[['Folk' '199']\n",
      " ['Hip-hop' '21']\n",
      " ['Indie' '14']\n",
      " ['Jazz' '2445']\n",
      " ['Metal' '3625']\n",
      " ['Pop' '17590']\n",
      " ['Rock' '26145']]\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model_otf.predict(X_val)\n",
    "\n",
    "labels = ['Country', 'Electric', 'Folk', 'Hip-hop', 'Indie', 'Jazz', 'Metal', 'Pop', 'R&B', 'Rock']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(50039):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joU4JBV7wZvW"
   },
   "source": [
    "Check the result on the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QX5zqcqwX8D",
    "outputId": "dece53e4-7196-4baa-c250-5c2946ce8bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.65836099e-03 5.51757077e-03 6.14404120e-02 ... 4.95431185e-01\n",
      "  1.51803158e-02 3.09573054e-01]\n",
      " [1.22261583e-03 4.29221289e-03 1.29306260e-02 ... 9.87532064e-02\n",
      "  1.63631234e-03 6.47214830e-01]\n",
      " [2.20787861e-05 4.12383210e-03 2.49847560e-03 ... 2.14751959e-02\n",
      "  3.43735592e-04 4.13438790e-02]\n",
      " ...\n",
      " [3.43920183e-05 6.48433901e-03 1.70925388e-03 ... 4.30818588e-01\n",
      "  3.84547631e-03 4.06638207e-03]\n",
      " [3.49153561e-04 5.87891182e-03 1.03386985e-02 ... 5.99887483e-02\n",
      "  3.82853003e-04 6.19561613e-01]\n",
      " [7.40548270e-03 3.19059379e-02 4.21678722e-02 ... 1.75882638e-01\n",
      "  4.84450068e-03 4.31513339e-01]]\n",
      "[['Folk' '67']\n",
      " ['Hip-hop' '16']\n",
      " ['Indie' '2']\n",
      " ['Jazz' '340']\n",
      " ['Metal' '832']\n",
      " ['Pop' '2704']\n",
      " ['Rock' '3974']]\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model_otf.predict(X_test)\n",
    "\n",
    "labels = ['Country', 'Electric', 'Folk', 'Hip-hop', 'Indie', 'Jazz', 'Metal', 'Pop', 'R&B', 'Rock']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(7935):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AweaaCYvTPP"
   },
   "source": [
    "## Best Model Using Pre-trained Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0-v-S8GvcXE"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fbn6rKgb6Zr",
    "outputId": "a3ba25df-8ad8-442d-b346-f20383c8593d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200157,), (50040,), (200157,), (50040,))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics = en_train['Lyrics'].values\n",
    "genre = en_train['Genre'].values\n",
    "lyrics_test= test['Lyrics'].values\n",
    "genre_test= test['Genre'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lyrics_train, lyrics_val, genre_train, genre_val = train_test_split(lyrics, genre, test_size=0.2, random_state=142)\n",
    "\n",
    "lyrics_train.shape, lyrics_val.shape, genre_train.shape, genre_val.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NncztENMazg8"
   },
   "source": [
    "Create a vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwBehqbCV3ly"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=5000, output_sequence_length=400)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(lyrics_train).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uxl6tOTHa_8u"
   },
   "source": [
    "retrieve the computed vocabulary used via vectorizer.get_vocabulary(). Let's print the top 7 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSgHgCEvaUG2",
    "outputId": "7207ea5b-0acd-4a70-ef37-92766ccb159c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'you', 'i', 'to', 'and']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uZ1Rgnfai3X"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDvaKREGDDH3"
   },
   "source": [
    "Encoding the Genre Column using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0DTwmBdr_3s"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(genre_train)\n",
    "y_train = encoder.transform(genre_train)\n",
    "y_val = encoder.transform(genre_val)\n",
    "y_test = encoder.transform(genre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ek83cQbTCfdF",
    "outputId": "2b12947c-7097-40eb-d340-982eefa4fd62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200157,), array([5, 7, 9, ..., 9, 9, 7]))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiGHMJT6Cms0",
    "outputId": "09df197f-6594-43f1-e2c3-5c50c332a470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  810]\n",
      " [   1  660]\n",
      " [   2  495]\n",
      " [   3  960]\n",
      " [   4  510]\n",
      " [   5  660]\n",
      " [   6  810]\n",
      " [   7 1110]\n",
      " [   8  510]\n",
      " [   9 1410]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_test, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kydSJ9kcDKPD"
   },
   "source": [
    "Vectorization for the Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31mmaKs6sSZ_"
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer(lyrics_train)  # Transforms each text in texts to a sequence of integers with its corresponding integer value from the word_index dictionary\n",
    "X_val = vectorizer(lyrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tjw-j1vd7jr"
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer(lyrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7tvuwI2tZXK",
    "outputId": "9e5c9be1-7a9f-4d27-ab72-47837c52059d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 100   85  100   35   17    2  199   63   46 1111  216   11 3510  594\n",
      "   45   11  341   28    4   25   15   78  224   13 1308   34   15   69\n",
      "   16  431   69   16  431   11 2787    4  123  100   85    6   76  766\n",
      "   76   10  341  529  114  199   41  223 2116    1   16    7  220   28\n",
      "    4   22   65   10   65   10   41    8   45   66   32  403 1524  104\n",
      "   10  619 1214  122   89   91    7 3961    7    1    7    1    7 4580\n",
      "    7 4923    6    7  539   89   91   31    6   45    6  124    6   43\n",
      "    6    4   25   47  174  421   48    4  135  240 2100   16    9  188\n",
      "    4  630  240   31    6   41   69   11    2 1117  100   85    4   88\n",
      "  144    4   56 1108   10    4  231   12    1   50   28    9   93   30\n",
      "   84   78  665   10    6   42    4  272  107   10   53  613   47  835\n",
      "  172  192  637   81   16    7  233  979    6  113  192  311   89   91\n",
      "    7 3961    7    1    7    1    7 4580    7 4923    6    7  539   89\n",
      "   91   31    6   45    6  124    6   43    6    4   25   47  174  421\n",
      "   48    4  135  240 2112 2100   16    9  188    4   30  630  240   31\n",
      "    6   41   69   11    2 1117  100   85  100   85    6    4   56 1108\n",
      "   10  309  306    4  231   12 3186   43   28    9   93  119  665   10\n",
      "   28   42  108  143 2027   60   32   82 3122   15   78  342  240   31\n",
      "   11    7  233  797    6  232    9    9    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(400,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bavpiL05ncaO",
    "outputId": "92fb181c-1717-4e15-f3d1-03df9c8f8d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "def FindMaxLength(lst):\n",
    "    \n",
    "    maxLength = max(len(x) for x in lst )\n",
    "  \n",
    "    return maxLength\n",
    "\n",
    "print(FindMaxLength(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVzFu49CbNc3"
   },
   "source": [
    "Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Znhstes_a1mH",
    "outputId": "a49da2c2-e58c-417f-fe68-c3b24b7a02e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-06 18:40:35--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-05-06 18:40:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-05-06 18:40:36--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 40s  \n",
      "\n",
      "2021-05-06 18:43:17 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfS4Ob3dJCb"
   },
   "source": [
    "dict mapping words (strings) to their NumPy vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOiYidHncPu1",
    "outputId": "940607c2-40d8-4f3b-ff59-7fdc18b96ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.200d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YGVBng-fWzr",
    "outputId": "85356c8d-dd01-4326-c548-3222aab40233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4882 words (118 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 1\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeoUNYKjw0rm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjbphOw5vgsd"
   },
   "source": [
    "### **Testing the best model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGlBq1_rTjeF"
   },
   "source": [
    "Best model is single layer LSTM with epoch 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjsvF2eYsDjD",
    "outputId": "68f40c2f-9f1e-4ace-f34c-9cee70434361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 200)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 1,363,010\n",
      "Trainable params: 1,363,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1766Zu75bDRb2CiJCuUR1nsGUTGtbnP0j', dest_path='./model/model_SingleL.h5')\n",
    "\n",
    "best_model_pret = tf.keras.models.load_model('./model/model_SingleL.h5')\n",
    "\n",
    "best_model_pret.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyLDhaDgs-pt",
    "outputId": "b48dcaf2-779b-4b38-c4a0-a57bbf55e13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6666\n",
      "validation Accuracy:  0.6260\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model_pret.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = best_model_pret.evaluate(X_val, y_val, verbose=False)\n",
    "print(\"validation Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nML5KaTEaEof",
    "outputId": "506c4276-9f18-41de-d0cf-6cbc802880fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.2858\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model_pret.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCvNzYqeylFj",
    "outputId": "9b753327-94a8-4594-b37a-6975a8c34452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0512111  0.00168165 0.10268727 ... 0.08583675 0.00912838 0.65284014]\n",
      " [0.00343351 0.01854887 0.04195912 ... 0.2574531  0.0162961  0.45860615]\n",
      " [0.05285006 0.00260134 0.22336395 ... 0.08038729 0.00685644 0.54957646]\n",
      " ...\n",
      " [0.01609807 0.00862634 0.04596556 ... 0.23237742 0.01086019 0.5774801 ]\n",
      " [0.00533066 0.02174569 0.02513797 ... 0.35325867 0.01696336 0.43985423]\n",
      " [0.00989971 0.01075727 0.03865423 ... 0.1647999  0.00960178 0.6488917 ]]\n",
      "[['Folk' '25']\n",
      " ['Hip-hop' '18']\n",
      " ['Jazz' '2302']\n",
      " ['Metal' '3549']\n",
      " ['Pop' '12707']\n",
      " ['Rock' '31438']]\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model_pret.predict(X_val)\n",
    "\n",
    "labels = ['Country', 'Electric', 'Folk', 'Hip-hop', 'Indie', 'Jazz', 'Metal', 'Pop', 'R&B', 'Rock']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(50039):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LmghJUGXnp3",
    "outputId": "a69b2f35-5e1a-4dad-d13a-dfad3e27add5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.5345044e-04 2.3709906e-04 2.2864187e-02 ... 1.2144128e-01\n",
      "  2.0890501e-03 8.3199215e-01]\n",
      " [1.5420245e-04 1.1312389e-02 6.6888952e-03 ... 3.5181698e-01\n",
      "  4.3151695e-03 4.5257539e-01]\n",
      " [3.8064285e-05 6.9802208e-03 3.0849972e-03 ... 2.1472165e-01\n",
      "  2.7510722e-03 5.7729048e-01]\n",
      " ...\n",
      " [2.4561009e-09 5.9412228e-05 6.1837360e-05 ... 3.6882529e-01\n",
      "  2.5487245e-05 8.6820006e-02]\n",
      " [2.7716463e-04 5.4966202e-03 1.2493230e-02 ... 1.2851885e-01\n",
      "  2.6247189e-03 7.1468717e-01]\n",
      " [7.0836756e-04 1.7914865e-02 1.6546553e-02 ... 1.6856678e-01\n",
      "  6.4815311e-03 5.7805604e-01]]\n",
      "[['Folk' '1']\n",
      " ['Hip-hop' '1']\n",
      " ['Jazz' '82']\n",
      " ['Metal' '207']\n",
      " ['Pop' '486']\n",
      " ['Rock' '1223']]\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model_pret.predict(X_test)\n",
    "\n",
    "labels = ['Country', 'Electric', 'Folk', 'Hip-hop', 'Indie', 'Jazz', 'Metal', 'Pop', 'R&B', 'Rock']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(2000):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTbtaWppv51N"
   },
   "source": [
    "# Part 2 -- Transfer Learning (Artist Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WN3mSaQzzP1"
   },
   "source": [
    "## Best Model Using Embedding trained on-the-fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc2slpR30Tlj"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7G8Ux2Xp4GL"
   },
   "outputs": [],
   "source": [
    "vertical_stack = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "eHpnRbLRq-LJ",
    "outputId": "f579c021-6c5b-4a19-c04c-50f4b443c4e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song year</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>world so cold</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>It starts with pain, followed by hate\\nFueled ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>broken</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Freedom!\\nAlone again again alone\\nPatiently w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>3 leaf loser</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Biting the hand that feeds you, lying to the v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>anthem for the underdog</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>You say you know just who I am\\nBut you can't ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>adrenaline</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>My heart is beating faster can't control these...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>amanda-marshall</td>\n",
       "      <td>too-little-too-late</td>\n",
       "      <td>Rock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tuesday night   hear voice telephone doin fine...</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>23453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>aurora-borealis</td>\n",
       "      <td>berserker</td>\n",
       "      <td>Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elite forces cloaked fur un sensitive pain bur...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2724.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>dr-dre</td>\n",
       "      <td>natural-born-killaz</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dr dre journey mind maniac doomed killer since...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>24147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>chris-cornell</td>\n",
       "      <td>wide-awake</td>\n",
       "      <td>Rock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>look hurricane right eye  people dead left die...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>4150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>the-coral</td>\n",
       "      <td>god-knows</td>\n",
       "      <td>Rock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deal truth till find proof feel weight step to...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>16992.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298118 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Artist                     Song  ... Song year Track_id\n",
       "0           12 stones            world so cold  ...       NaN      NaN\n",
       "1           12 stones                   broken  ...       NaN      NaN\n",
       "2           12 stones             3 leaf loser  ...       NaN      NaN\n",
       "3           12 stones  anthem for the underdog  ...       NaN      NaN\n",
       "4           12 stones               adrenaline  ...       NaN      NaN\n",
       "...               ...                      ...  ...       ...      ...\n",
       "7930  amanda-marshall      too-little-too-late  ...    2006.0  23453.0\n",
       "7931  aurora-borealis                berserker  ...    2007.0   2724.0\n",
       "7932           dr-dre      natural-born-killaz  ...    2010.0  24147.0\n",
       "7933    chris-cornell               wide-awake  ...    2011.0   4150.0\n",
       "7934        the-coral                god-knows  ...    2007.0  16992.0\n",
       "\n",
       "[298118 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertical_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1olIY5Hq_SK",
    "outputId": "76280c48-5906-463d-bfbf-ef2ae01169b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elvis presley         1611\n",
       "chris brown           1239\n",
       "elvis costello         923\n",
       "ella fitzgerald        874\n",
       "the rolling stones     820\n",
       "bee gees               811\n",
       "glee                   765\n",
       "bad religion           752\n",
       "beyonce                752\n",
       "elton john             728\n",
       "Name: Artist, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertical_stack['Artist'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Ao449uCErtvW",
    "outputId": "de851065-571a-42d6-deac-9835907376a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song year</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>american jesus</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>I don't need to be a global citizen,\\n'Cuz I'm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>infected</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Infected\\nNow here I go,\\nHope I don't break d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>a walk</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>I'm going for a walk\\nnot the after dinner kin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>sorrow</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Father can you hear me?\\nHow have I let you do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>you</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>There's a place where everyone can be happy\\nI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289878</th>\n",
       "      <td>ella fitzgerald</td>\n",
       "      <td>too marvelous for words</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>en</td>\n",
       "      <td>You're just too marvelous Too marvelous for wo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289948</th>\n",
       "      <td>chris brown</td>\n",
       "      <td>i get around</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>en</td>\n",
       "      <td>Up, high, down, low I be anywhere you want Not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290027</th>\n",
       "      <td>ella fitzgerald</td>\n",
       "      <td>ev rything i ve got</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>en</td>\n",
       "      <td>Don't stamp your foot at me, It's impolite To ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>beyonce</td>\n",
       "      <td>6-inch</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>six inch heels walked club like nobody busines...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>5220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7431</th>\n",
       "      <td>beyonce</td>\n",
       "      <td>sweet-dreams</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>turn lights every night rush bed hopes maybe g...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>13805.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9275 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Artist                     Song  ... Song year Track_id\n",
       "3791       bad religion           american jesus  ...       NaN      NaN\n",
       "3792       bad religion                 infected  ...       NaN      NaN\n",
       "3793       bad religion                   a walk  ...       NaN      NaN\n",
       "3794       bad religion                   sorrow  ...       NaN      NaN\n",
       "3795       bad religion                      you  ...       NaN      NaN\n",
       "...                 ...                      ...  ...       ...      ...\n",
       "289878  ella fitzgerald  too marvelous for words  ...       NaN      NaN\n",
       "289948      chris brown             i get around  ...       NaN      NaN\n",
       "290027  ella fitzgerald      ev rything i ve got  ...       NaN      NaN\n",
       "1574            beyonce                   6-inch  ...    2016.0   5220.0\n",
       "7431            beyonce             sweet-dreams  ...    2012.0  13805.0\n",
       "\n",
       "[9275 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = ['elvis presley','chris brown','elvis costello','ella fitzgerald','the rolling stones', 'bee gees', 'glee', 'beyonce', 'bad religion', 'elton john']\n",
    "top10 = vertical_stack[vertical_stack['Artist'].isin(artists)]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_NHJz35yH9-",
    "outputId": "2d7f59d5-853f-47f3-dc4f-ec4b490645d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "top10['Lyrics'] = top10['Lyrics'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BVK9p5m1r6j",
    "outputId": "825c3251-da0f-4820-db58-62b9fc764e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "remove_punc = re.compile('[^0-9a-z ]') # removing any symbol that is NOT number, letter, space.\n",
    "STOPWORDS = set(stopwords.words('english')) # only dealing with English Lyrics, so set stopwords to 'english'\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = text.lower() # lowercase text\n",
    "    text = remove_punc.sub(' ', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with space. \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text\n",
    "\n",
    "top10['Lyrics'] = top10['Lyrics'].apply(clean_text)\n",
    "top10['Lyrics'] = top10['Lyrics'].str.replace('\\d+', '') # replacing one or more digits by nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "TpQceaFgzLPM",
    "outputId": "ed63d0c6-64be-45d2-d72d-0a07090fd045"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song year</th>\n",
       "      <th>Track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>american jesus</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>need global citizen cuz blessed nationality me...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>infected</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>infected go hope break take anything need anyt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>a walk</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>going walk dinner kind gonna use hands gonna u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>sorrow</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>father hear let curse day born sorrow world le...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>bad religion</td>\n",
       "      <td>you</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>place everyone happy beautiful place whole fuc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289878</th>\n",
       "      <td>ella fitzgerald</td>\n",
       "      <td>too marvelous for words</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>en</td>\n",
       "      <td>marvelous marvelous words like glorious glamor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289948</th>\n",
       "      <td>chris brown</td>\n",
       "      <td>i get around</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>en</td>\n",
       "      <td>high low anywhere want one slipping sliding yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290027</th>\n",
       "      <td>ella fitzgerald</td>\n",
       "      <td>ev rything i ve got</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>en</td>\n",
       "      <td>stamp foot impolite stamp foot quite right man...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>beyonce</td>\n",
       "      <td>6-inch</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>six inch heels walked club like nobody busines...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>5220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7431</th>\n",
       "      <td>beyonce</td>\n",
       "      <td>sweet-dreams</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>turn lights every night rush bed hopes maybe g...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>13805.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9275 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Artist                     Song  ... Song year Track_id\n",
       "3791       bad religion           american jesus  ...       NaN      NaN\n",
       "3792       bad religion                 infected  ...       NaN      NaN\n",
       "3793       bad religion                   a walk  ...       NaN      NaN\n",
       "3794       bad religion                   sorrow  ...       NaN      NaN\n",
       "3795       bad religion                      you  ...       NaN      NaN\n",
       "...                 ...                      ...  ...       ...      ...\n",
       "289878  ella fitzgerald  too marvelous for words  ...       NaN      NaN\n",
       "289948      chris brown             i get around  ...       NaN      NaN\n",
       "290027  ella fitzgerald      ev rything i ve got  ...       NaN      NaN\n",
       "1574            beyonce                   6-inch  ...    2016.0   5220.0\n",
       "7431            beyonce             sweet-dreams  ...    2012.0  13805.0\n",
       "\n",
       "[9275 rows x 7 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnZx8lhwtItP"
   },
   "outputs": [],
   "source": [
    "lyrics = top10['Lyrics'].values\n",
    "art = top10['Artist'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lyrics_train, lyrics_val, art_train, art_val = train_test_split(lyrics, art, test_size=0.1, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5xqLPOrzswP",
    "outputId": "bf105df2-17a4-4b79-883d-9dfd2ecf1d03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hahaha ya wanna learn ya abcs abc haha x ya gon like right many ways spell love one way show one way show scare mountain yell til face turn blue come back prove prove would rather let day day actions speak louder say pick letters alphabetbet would say way maybe b something c def initely know want g take hi higher j tryna say love simple abc gotta sing since made clear understand wanna wanna know cool would hurt said know cause p q x ed rest man got eyes u would rather let day day actions speak louder say pick letters alphabetbet would say way maybe b something c def initely know want g take hi higher j tryna say love simple abc',\n",
       "       'systems life hum accordingly every day every year every century everywhere humans go things get worse worse tell smoke dirty city jungle tranquil hideaway king trees mountains man things sell million dollars pound cut dry good display shelves leave alone animal dangerous beast ones made know feast  lone souls breed losing generation seems killers know blame man plan frightening greed',\n",
       "       'oh know mad fool like girls done many nights time time life strange know somebody somebody cry gonna dancing get home dancing gonna dancing gonna dancing telling every little thing done glanced jackets paperbacks read every one drag insane everybody feel pain gonna dancing get home getting knees finds brother easy please nights paper striptease caught like disease says dancing gonna dancing even speak face says even though wanna shake hand ever bow know see give anythin sympathy gonna dancing get home dancing gonna dancing',\n",
       "       ...,\n",
       "       'one two three four somewhere downtown pretty girl kneels offers soft lips handful pills peels dress rest skills buys wants rest steals speaks deep swallows rum head beating like big bass drum wishes mute dumb trick asked quick come chorus drag saluting starry rag rather go blind speaking mind use like gag raise anger let hang american gangster time sits back starts invent saigon correspondent til carbine fell silent spent never knew could eloquent next week fashionable new sin harlot puritan pull wings stick pin watch money roll chorus got hidden sleeve tracks train bidding leave say flatter deceive count reprieve hands helpless raised dead little secrets praised people stand dumbstruck dazed inches erased chorus committing perfect crime american gangster time go bye bye american gangster time',\n",
       "       'little mama said one day find love last eternally want everywhere go please leave baby love ruler whole wide world say girl want everything know need baby love mountain climb find mountain child find time want night day well make happy baby go away swim ocean raging foam know come back home want know love well want baby never let go well want baby never let go',\n",
       "       'oh love darling hungered touch long lonely time time goes slowly time much still mine need love need love god speed love lonely rivers flow sea sea open arms sea yeah lonely rivers sigh wait wait coming home wait oh love darling hungered hungered touch long lonely time time goes slowly time much still mine need love need love god speed love windson effgen'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hboq8kiR0EXq",
    "outputId": "3dec708c-b880-4e4d-e2bf-2f0599768d7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chris brown', 'bad religion', 'elvis costello', ...,\n",
       "       'elvis costello', 'elvis presley', 'glee'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9Lo8sF2unZC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(art_train)\n",
    "y_train = encoder.transform(art_train)\n",
    "y_val = encoder.transform(art_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpUS2waknRJC",
    "outputId": "97688c85-2db9-4507-a78f-729a5805a9d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bad religion' 688]\n",
      " ['bee gees' 723]\n",
      " ['beyonce' 679]\n",
      " ['chris brown' 1133]\n",
      " ['ella fitzgerald' 784]\n",
      " ['elton john' 645]\n",
      " ['elvis costello' 818]\n",
      " ['elvis presley' 1459]\n",
      " ['glee' 677]\n",
      " ['the rolling stones' 741]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(art_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Txx6Hzxzu_Wt",
    "outputId": "c8af3067-c97d-4f69-aabe-514d97f3a277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  688]\n",
      " [   1  723]\n",
      " [   2  679]\n",
      " [   3 1133]\n",
      " [   4  784]\n",
      " [   5  645]\n",
      " [   6  818]\n",
      " [   7 1459]\n",
      " [   8  677]\n",
      " [   9  741]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ic7m3WNU7ppk",
    "outputId": "1c1d26f5-df8b-40ab-df75-9c9dc3180ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  64]\n",
      " [  1  88]\n",
      " [  2  73]\n",
      " [  3 106]\n",
      " [  4  90]\n",
      " [  5  83]\n",
      " [  6 105]\n",
      " [  7 152]\n",
      " [  8  88]\n",
      " [  9  79]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_val, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKyQJ4TG7qDZ",
    "outputId": "3dfe9b6f-3894-4a6f-d044-5d6652cf24ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8347, 10), (928, 10))"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "dummy_y_train = np_utils.to_categorical(y_train)\n",
    "dummy_y_val = np_utils.to_categorical(y_val)\n",
    "\n",
    "dummy_y_train.shape, dummy_y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIIM4SRc7qdn"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer = Tokenizer(num_words=5000)   # num_words is the size of the vocabulary (top 5000 frequent words in the vocabulary)\n",
    "tokenizer.fit_on_texts(lyrics_train)    # Updates internal vocabulary based on a list of texts\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(lyrics_train)  # Transforms each text in texts to a sequence of integers with its corresponding integer value from the word_index dictionary\n",
    "X_val = tokenizer.texts_to_sequences(lyrics_val)\n",
    "\n",
    "# The number of unique words in the whole training text\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index for padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6cFRPCD0x2U",
    "outputId": "1bb9355e-1eed-49c4-e75e-9dcafac0815d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24954"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xs4JyvrL7qn_",
    "outputId": "8fa84017-3ec9-437b-ccfe-d7f6a497d555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4978, 46, 38, 4978, 46, 38, 4978, 46, 38, 1665, 2388, 114, 3, 411, 241, 94, 87, 46, 38, 87, 46, 38, 87, 46, 38, 261, 114, 30, 80, 18, 505, 816, 129, 27, 832, 1179, 299, 1421, 248, 83, 96, 4869, 315, 1047, 50, 792, 83, 309, 309, 309, 309, 309, 171, 1944, 2065, 231, 26, 60, 241, 94, 87, 46, 38, 87, 46, 38, 87, 46, 38, 309, 309, 309, 171, 1944, 2065, 231, 26, 60, 241, 94, 87, 46, 38, 87, 46, 38, 87, 46, 38, 87, 46, 87, 46, 38], 681)\n",
      "([4702, 438, 117, 402, 1045, 4, 4702, 438, 30, 115, 816, 39, 38, 406, 218, 143, 974, 4, 4702, 438, 141, 30, 9, 4, 9, 2, 2, 29, 4702, 438, 117, 402, 1045, 4, 4702, 438, 141, 30, 115, 4, 4, 4, 4, 4702, 438, 141, 30, 115, 4, 4, 4, 4, 4702, 438, 141, 30, 115], 681)\n"
     ]
    }
   ],
   "source": [
    "def FindMaxLength(lst):\n",
    "    maxList = max((x) for x in lst)\n",
    "    maxLength = max(len(x) for x in lst )\n",
    "  \n",
    "    return maxList, maxLength\n",
    "\n",
    "print(FindMaxLength(X_train))\n",
    "print(FindMaxLength(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kl4dSRd17qza",
    "outputId": "85ff2c00-c8a5-4110-94ea-2c1d3b43f0a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8347, 500), (928, 500))"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 500\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0TQnxbn0fHm"
   },
   "source": [
    "### The best model built from scratch (with on-the-fly embedding)\n",
    "\n",
    "The best model is CNN trained with 7 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pE7tZEC022Eq",
    "outputId": "e8da03b7-18be-40de-cc01-907e138b17ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1UdxZZCljf8Roq6fBFwnMOaaJh1SzZzBb into ./model/tl_cnn_epoch7.h5... Done.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 498, 128)          19328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 297,138\n",
      "Trainable params: 297,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1UdxZZCljf8Roq6fBFwnMOaaJh1SzZzBb', dest_path='./model/tl_cnn_epoch7.h5')\n",
    "\n",
    "scratch_otf = tf.keras.models.load_model('./model/tl_cnn_epoch7.h5')\n",
    "\n",
    "scratch_otf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f_JlJ6122Ev",
    "outputId": "e5f8546c-3e15-40b0-b940-d6e38fa1870d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Accuracy:  0.9903\n",
      "Validation Dataset Accuracy:  0.7909\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = scratch_otf.evaluate(X_train, dummy_y_train, verbose=False)\n",
    "print(\"Training Dataset Accuracy:  {:.4f}\".format(accuracy))\n",
    "loss, accuracy = scratch_otf.evaluate(X_val, dummy_y_val, verbose=False)\n",
    "print(\"Validation Dataset Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5-ex5ja22Ev"
   },
   "source": [
    "Check the result on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hNaZ7qE22Ev",
    "outputId": "5ee1f5b4-4dee-40fe-8953-cc9928b2d260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5320094e-07 6.5559962e-06 4.3202783e-03 ... 2.3106678e-11\n",
      "  1.3974401e-04 2.0110960e-07]\n",
      " [9.9080771e-01 3.7443021e-03 1.9111194e-06 ... 2.7092715e-06\n",
      "  1.1338397e-03 4.5745386e-05]\n",
      " [1.5824487e-03 2.7158063e-02 1.6904833e-02 ... 1.3712190e-02\n",
      "  4.6609238e-02 2.5653485e-03]\n",
      " ...\n",
      " [7.9416146e-04 4.0368345e-03 9.9486322e-04 ... 1.3157184e-03\n",
      "  1.0465756e-02 2.1141142e-04]\n",
      " [3.5550676e-09 5.3796880e-03 1.0640523e-04 ... 9.8373377e-01\n",
      "  2.6470004e-04 1.0302851e-02]\n",
      " [4.4256987e-05 1.5027550e-02 1.6775455e-03 ... 2.4237382e-01\n",
      "  3.3530775e-01 7.3352732e-02]]\n",
      "[['bad religion' '684']\n",
      " ['bee gees' '724']\n",
      " ['beyonce' '682']\n",
      " ['chris brown' '1134']\n",
      " ['ella fitzgerald' '792']\n",
      " ['elton john' '651']\n",
      " ['elvis costello' '818']\n",
      " ['elvis presley' '1464']\n",
      " ['glee' '654']\n",
      " ['the rolling stones' '744']]\n"
     ]
    }
   ],
   "source": [
    "prediction = scratch_otf.predict(X_train)\n",
    "\n",
    "labels = ['bad religion', 'bee gees', 'beyonce', 'chris brown', 'ella fitzgerald', 'elton john', 'elvis costello', 'elvis presley', 'glee', 'the rolling stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(8347):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWNqOOWG22Ew"
   },
   "source": [
    "Check the result on the Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJAijmRA22Ew",
    "outputId": "9fa8a3fc-6ccb-49be-bd43-1b1f8c9d6cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.84987561e-11 2.64233861e-08 4.29397733e-06 ... 4.22148244e-14\n",
      "  5.30774514e-07 2.78395285e-09]\n",
      " [1.41349407e-02 1.62381425e-01 1.17883570e-01 ... 1.14015285e-02\n",
      "  4.11766112e-01 5.36019867e-03]\n",
      " [1.32799204e-02 1.71347663e-01 3.35061550e-02 ... 1.63154153e-03\n",
      "  1.24622166e-01 6.99631113e-04]\n",
      " ...\n",
      " [1.07996730e-05 9.38685417e-01 1.42218275e-02 ... 7.04260485e-04\n",
      "  2.48424765e-02 2.18916894e-03]\n",
      " [7.01721819e-06 9.95482087e-01 7.30490865e-05 ... 9.01784661e-05\n",
      "  8.43149377e-04 5.70072676e-04]\n",
      " [9.94126201e-01 3.49222799e-04 9.02818374e-06 ... 1.97654663e-06\n",
      "  3.61830997e-03 6.74436742e-05]]\n",
      "[['bad religion' '66']\n",
      " ['bee gees' '89']\n",
      " ['beyonce' '77']\n",
      " ['chris brown' '109']\n",
      " ['ella fitzgerald' '99']\n",
      " ['elton john' '78']\n",
      " ['elvis costello' '108']\n",
      " ['elvis presley' '154']\n",
      " ['glee' '73']\n",
      " ['the rolling stones' '75']]\n"
     ]
    }
   ],
   "source": [
    "prediction = scratch_otf.predict(X_val)\n",
    "\n",
    "labels = ['bad religion', 'bee gees', 'beyonce', 'chris brown', 'ella fitzgerald', 'elton john', 'elvis costello', 'elvis presley', 'glee', 'the rolling stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(928):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woM-wdFF06YN"
   },
   "source": [
    "### The best pre-trained model (with on-the-fly embedding) \n",
    "the best pre-trained model is CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXzOM0as5O7O"
   },
   "source": [
    "`No Fine Tuning of hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el52R5db6tXG",
    "outputId": "7c334616-90e3-401b-e122-c070f273b225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 128)          19328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 297,138\n",
      "Trainable params: 297,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1-9LYc6KNeX5Yehjk2I4vURPLm6Z_2hoK', dest_path='./model/CNN_model_epoch4.h5')\n",
    "\n",
    "best_model_otf = tf.keras.models.load_model('./model/CNN_model_epoch4.h5')\n",
    "\n",
    "best_model_otf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ppFBoFo7JjU",
    "outputId": "7cb5145b-d061-4dc3-b028-ec07d0a0a783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training datset Accuracy:  0.1324\n",
      "Validation dataset Accuracy:  0.1207\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model_otf.evaluate(X_train, dummy_y_train, verbose=False)\n",
    "print(\"Training datset Accuracy:  {:.4f}\".format(accuracy))\n",
    "loss, accuracy = best_model_otf.evaluate(X_val, dummy_y_val, verbose=False)\n",
    "print(\"Validation dataset Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGjeFBoS7Nd0",
    "outputId": "0319630f-c5b4-41fa-c54b-e99ba75b2732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69143550e-05 7.97159784e-03 4.73974855e-04 ... 7.49000192e-01\n",
      "  2.93304282e-03 3.02456524e-02]\n",
      " [1.88999006e-03 2.07444597e-02 1.62274297e-02 ... 6.19106114e-01\n",
      "  1.39653692e-02 1.12237409e-01]\n",
      " [2.25783698e-03 6.65085111e-03 9.51187965e-03 ... 6.27417505e-01\n",
      "  4.28851172e-02 2.37856768e-02]\n",
      " ...\n",
      " [3.95112671e-02 7.02405639e-04 4.89297807e-02 ... 5.60951009e-02\n",
      "  1.07633760e-02 7.80776143e-01]\n",
      " [1.58544618e-03 1.65936947e-02 6.30305987e-03 ... 4.77492034e-01\n",
      "  5.12489630e-03 3.82561743e-01]\n",
      " [1.06670745e-02 8.65931995e-03 7.15162233e-02 ... 3.58107507e-01\n",
      "  2.43422873e-02 1.17174104e-01]]\n",
      "[['beyonce' '1']\n",
      " ['elton john' '10']\n",
      " ['elvis costello' '14']\n",
      " ['elvis presley' '558']\n",
      " ['the rolling stones' '345']]\n"
     ]
    }
   ],
   "source": [
    "prediction = best_model_otf.predict(X_val)\n",
    "\n",
    "labels = ['bad religion', 'bee gees', 'beyonce', 'chris brown', 'ella fitzgerald', 'elton john', 'elvis costello', 'elvis presley', 'glee', 'the rolling stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(928):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z0xX2GP7ngD"
   },
   "source": [
    "`With Fine Tuning of the Hyperparameters`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJAXtEyb8ILt",
    "outputId": "63853b03-4d28-44ac-ccd4-9531cd82926a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1SJzE0D99TrjshiKmM4ZIaxAtUn1juFC_ into ./model/tl_cnn_retrain_outputlayer.h5... Done.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 128)          19328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 297,138\n",
      "Trainable params: 2,010\n",
      "Non-trainable params: 295,128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1SJzE0D99TrjshiKmM4ZIaxAtUn1juFC_', dest_path='./model/tl_cnn_retrain_outputlayer.h5')\n",
    "\n",
    "ft_model_otf = tf.keras.models.load_model('./model/tl_cnn_retrain_outputlayer.h5')\n",
    "\n",
    "ft_model_otf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtGHw9Y_8IL5",
    "outputId": "91a9f473-931f-4e78-dea0-1c1cbd92f23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training datset Accuracy:  0.9909\n",
      "Validation dataset Accuracy:  0.7909\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = ft_model_otf.evaluate(X_train, dummy_y_train, verbose=False)\n",
    "print(\"Training datset Accuracy:  {:.4f}\".format(accuracy))\n",
    "loss, accuracy = ft_model_otf.evaluate(X_val, dummy_y_val, verbose=False)\n",
    "print(\"Validation dataset Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2jjyvjl8IL5",
    "outputId": "cd7182e1-4df2-44da-823d-e5ab83905380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.58923396e-14 1.23968277e-06 8.85190384e-04 ... 2.28080223e-15\n",
      "  2.88731215e-04 8.30494074e-09]\n",
      " [4.47409292e-07 5.54348886e-01 1.83023240e-05 ... 1.99902263e-02\n",
      "  4.21032786e-01 5.24050629e-05]\n",
      " [1.17936315e-06 1.02256909e-01 7.38651585e-03 ... 4.52942113e-05\n",
      "  3.14319640e-01 6.33086562e-02]\n",
      " ...\n",
      " [4.73536730e-08 6.73604012e-01 1.11226555e-05 ... 1.41808335e-02\n",
      "  3.08149517e-01 2.57797161e-04]\n",
      " [1.92605003e-12 9.48007882e-01 5.53565194e-09 ... 4.00142446e-02\n",
      "  1.88750203e-03 1.33949789e-05]\n",
      " [9.12108898e-01 9.29680828e-05 7.16246804e-03 ... 9.27623551e-05\n",
      "  1.13965175e-03 6.20046444e-03]]\n",
      "[['bad religion' '66']\n",
      " ['bee gees' '114']\n",
      " ['beyonce' '72']\n",
      " ['chris brown' '106']\n",
      " ['ella fitzgerald' '78']\n",
      " ['elton john' '74']\n",
      " ['elvis costello' '91']\n",
      " ['elvis presley' '158']\n",
      " ['glee' '84']\n",
      " ['the rolling stones' '85']]\n"
     ]
    }
   ],
   "source": [
    "prediction = ft_model_otf.predict(X_val)\n",
    "\n",
    "labels = ['bad religion', 'bee gees', 'beyonce', 'chris brown', 'ella fitzgerald', 'elton john', 'elvis costello', 'elvis presley', 'glee', 'the rolling stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(928):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXCTCuAR81Sa"
   },
   "source": [
    "## Best Model Using Pre-trained Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQMLSURp9QP2"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U44vAwM5ukFG"
   },
   "outputs": [],
   "source": [
    "top10['Lyrics'] = top10['Lyrics'].astype(str)\n",
    "lyrics = top10['Lyrics'].values\n",
    "artist = top10['Artist'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT1GS16wvHoz",
    "outputId": "bb29e45a-3bc3-410b-ef61-173f43f28c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8347,), (928,), (8347,), (928,))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lyrics_train, lyrics_val, artist_train, artist_val = train_test_split(lyrics, artist, test_size=0.1, random_state=100)\n",
    "\n",
    "lyrics_train.shape, lyrics_val.shape, artist_train.shape, artist_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFEYUa7avs3A"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=5000, output_sequence_length=400)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(lyrics_train).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmkIxt7ByQtu",
    "outputId": "68d47096-27db-40d7-be92-c1a39082ad87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'you', 'i', 'the']"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2sY2Rxxvvrr"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1AxEIXLvyjN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(artist_train)\n",
    "y_train = encoder.transform(artist_train)\n",
    "y_val = encoder.transform(artist_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5MctgyP0lwe"
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer(lyrics_train)  # Transforms each text in texts to a sequence of integers with its corresponding integer value from the word_index dictionary\n",
    "X_val = vectorizer(lyrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ItOLJMV04ZZ",
    "outputId": "f0cb59ed-ebce-4b37-a293-41bf5c437131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.200d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKXxlUQe1TLD",
    "outputId": "15c63986-d70f-4667-9578-1265581a1c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4823 words (177 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 1\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zE3jbxw1UIP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVQlfr_H9VcH"
   },
   "source": [
    "### The Best Model built from Scratch (with pre-trained embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_LLChfj_jxW",
    "outputId": "17b59262-2eaa-4396-9089-67c7d42aa10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 400, 200)          1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 1,101,310\n",
      "Trainable params: 1,101,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "gdd.download_file_from_google_drive(file_id='1kUigTOFrNfxGPzQoLsuhN4dXEJa1w5Al', dest_path='./model/modelrnnTF.h5')\n",
    "model = tf.keras.models.load_model('./model/modelrnnTF.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvCxfaOnybL1",
    "outputId": "81f409e0-9a2c-41bc-89d1-d49b2b84a136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.2152\n",
      "validation Accuracy:  0.2047\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n",
    "print(\"validation Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OF2lZyplQjSV",
    "outputId": "430bbcd2-4857-4239-fe14-4bbae3e4e605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05870477 0.13421161 0.04597966 ... 0.23990214 0.10399468 0.09348586]\n",
      " [0.05870477 0.13421161 0.04597966 ... 0.23990214 0.10399468 0.09348586]\n",
      " [0.05870477 0.13421161 0.04597966 ... 0.23990214 0.10399468 0.09348586]\n",
      " ...\n",
      " [0.05870477 0.13421163 0.04597966 ... 0.23990214 0.10399466 0.09348585]\n",
      " [0.05870477 0.13421161 0.04597966 ... 0.23990211 0.10399466 0.09348587]\n",
      " [0.05870477 0.13421161 0.04597966 ... 0.23990214 0.10399468 0.09348586]]\n",
      "[['beyonce' '196']\n",
      " ['chris_brown' '52']\n",
      " ['elvis_costello' '1']\n",
      " ['elvis_presley' '1751']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = model.predict(X_train)\n",
    "\n",
    "labels = ['bad_religion', 'bee_gees', 'beyonce', 'chris_brown', 'ella_fitzgerald', 'elton_john', 'elvis_costello', 'elvis_presley', 'glee', 'the_rolling_stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(2000):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgH6mmY1QQPu",
    "outputId": "ad8f24cd-4047-4e5a-e596-5ee8c05842b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00622783 0.0077385  0.51898175 ... 0.02311949 0.13300845 0.03021261]\n",
      " [0.05870477 0.13421158 0.04597966 ... 0.23990214 0.10399468 0.09348586]\n",
      " [0.05870477 0.13421163 0.04597966 ... 0.23990211 0.10399468 0.09348586]\n",
      " ...\n",
      " [0.05945627 0.13461444 0.0443826  ... 0.24640162 0.09759777 0.09739693]\n",
      " [0.05870477 0.13421158 0.04597966 ... 0.23990214 0.10399466 0.09348585]\n",
      " [0.05870477 0.13421161 0.04597966 ... 0.23990214 0.10399467 0.09348586]]\n",
      "[['beyonce' '70']\n",
      " ['chris_brown' '15']\n",
      " ['elvis_presley' '615']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = model.predict(X_val)\n",
    "\n",
    "labels = ['bad_religion', 'bee_gees', 'beyonce', 'chris_brown', 'ella_fitzgerald', 'elton_john', 'elvis_costello', 'elvis_presley', 'glee', 'the_rolling_stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(700):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmQ3mloD9dlI"
   },
   "source": [
    "### The best pre-trained Model (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSEAgG2QcQ3Y",
    "outputId": "37915f8a-341e-4195-e7b7-342627374e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1IJwr8FTxZ2-47pcP239AtSIK88lmQKLX into ./model/lstmtf.h5... Done.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 200)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 1,363,010\n",
      "Trainable params: 1,363,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1IJwr8FTxZ2-47pcP239AtSIK88lmQKLX', dest_path='./model/lstmtf.h5')\n",
    "\n",
    "ft_model = tf.keras.models.load_model('./model/lstmtf.h5')\n",
    "\n",
    "ft_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMlmniKnMoLG",
    "outputId": "c0b91cee-9c69-4ae1-d067-8d3d65f235f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7968\n",
      "validation Accuracy:  0.5991\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = ft_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = ft_model.evaluate(X_val, y_val, verbose=False)\n",
    "print(\"validation Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rm-Df7b5MtJ4",
    "outputId": "47c09c4d-15ba-4f92-a27f-6cad8cfd30ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2199609e-08 1.4094957e-03 1.1857445e-03 ... 1.4755252e-03\n",
      "  1.1122129e-03 8.8469608e-04]\n",
      " [9.7155082e-01 6.9270953e-07 7.8308638e-03 ... 9.9372948e-05\n",
      "  8.9866859e-05 4.3132538e-03]\n",
      " [9.4115734e-03 1.9849463e-01 6.2013965e-02 ... 5.2551989e-02\n",
      "  3.0951244e-01 9.9035040e-02]\n",
      " ...\n",
      " [2.5030726e-05 9.3926296e-02 6.3269455e-03 ... 3.8149808e-02\n",
      "  1.3873946e-02 5.3044189e-02]\n",
      " [4.0823483e-10 1.3095730e-04 3.4517459e-08 ... 9.9801469e-01\n",
      "  5.5161794e-04 2.2798097e-04]\n",
      " [4.8080565e-05 3.6401615e-01 1.4242223e-03 ... 2.7544230e-01\n",
      "  1.3833526e-01 1.8551730e-02]]\n",
      "[['bad religion' '709']\n",
      " ['bee gees' '909']\n",
      " ['beyonce' '664']\n",
      " ['chris brown' '1178']\n",
      " ['ella fitzgerald' '824']\n",
      " ['elton john' '572']\n",
      " ['elvis costello' '747']\n",
      " ['elvis presley' '1435']\n",
      " ['glee' '839']\n",
      " ['the rolling stones' '470']]\n"
     ]
    }
   ],
   "source": [
    "prediction = ft_model.predict(X_train)\n",
    "\n",
    "labels = ['bad religion', 'bee gees', 'beyonce', 'chris brown', 'ella fitzgerald', 'elton john', 'elvis costello', 'elvis presley', 'glee', 'the rolling stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(8347):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "\n",
    "\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFN0Jd6WO6yX",
    "outputId": "51ed9c57-07fb-4f6d-bf22-1e77dbe73c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.20740323e-19 5.16710156e-12 1.03835891e-10 ... 5.98329570e-05\n",
      "  1.43649270e-08 2.17969571e-08]\n",
      " [1.09899374e-04 3.66530657e-01 2.34725308e-02 ... 5.67903146e-02\n",
      "  1.18681237e-01 3.22115310e-02]\n",
      " [1.59461226e-04 5.11186540e-01 4.82579134e-03 ... 5.74337281e-02\n",
      "  1.24350727e-01 3.61848772e-02]\n",
      " ...\n",
      " [4.73456166e-04 2.01499149e-01 2.03059092e-02 ... 3.84292863e-02\n",
      "  6.05450869e-01 4.98366654e-02]\n",
      " [9.19759199e-02 9.77465734e-02 1.22665249e-01 ... 6.51062727e-02\n",
      "  1.15334108e-01 1.30282968e-01]\n",
      " [7.99741685e-01 1.35854702e-04 6.09875135e-02 ... 2.43738946e-03\n",
      "  2.29082280e-03 3.06914914e-02]]\n",
      "[['bad_religion' '56']\n",
      " ['bee_gees' '84']\n",
      " ['beyonce' '57']\n",
      " ['chris_brown' '85']\n",
      " ['ella_fitzgerald' '79']\n",
      " ['elton_john' '51']\n",
      " ['elvis_costello' '74']\n",
      " ['elvis_presley' '113']\n",
      " ['glee' '70']\n",
      " ['the_rolling_stones' '31']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = ft_model.predict(X_val)\n",
    "\n",
    "labels = ['bad_religion', 'bee_gees', 'beyonce', 'chris_brown', 'ella_fitzgerald', 'elton_john', 'elvis_costello', 'elvis_presley', 'glee', 'the_rolling_stones']\n",
    "print(prediction)\n",
    "pred_str = []\n",
    "for i in range(700):\n",
    "  pred_str.append(labels[np.argmax(prediction[i])])\n",
    "(unique, counts) = np.unique(pred_str, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtpvRaTc9mke"
   },
   "source": [
    "# Part 3 -- Lyrics Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hJagWaj9sSS"
   },
   "source": [
    "## Rock Lyrics Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfV43ah6-nJt"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiE2OiTZrXuZ",
    "outputId": "bbb33f26-1205-47f2-85cb-6151436ab7dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          121404\n",
       "Pop           108714\n",
       "Metal          20291\n",
       "Jazz           13545\n",
       "Folk            8644\n",
       "Indie           8449\n",
       "R&B             2793\n",
       "Hip-Hop         2240\n",
       "Electronic      2213\n",
       "Country         1890\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I6ZvGxYHRnN"
   },
   "outputs": [],
   "source": [
    "!pip install -q profanity-check\n",
    "from profanity_check import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlPRrMCTrmUt",
    "outputId": "7761033b-4aa3-4216-8ac6-4cd79e9d4fec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((121404, 5), (108714, 5), (20291, 5))"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rock = train[train['Genre']=='Rock']\n",
    "pop = train[train['Genre']=='Pop']\n",
    "metal = train[train['Genre']=='Metal']\n",
    "\n",
    "rock.shape, pop.shape, metal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKw5Yu3HzWE6"
   },
   "outputs": [],
   "source": [
    "df1=pd.Dataframe(rock,columns=['lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVJpdy7gz8qz"
   },
   "source": [
    "cleaning offensive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkeAEfyfWeOl"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "actual_count=0\n",
    "for i in range(1000):\n",
    "  #print(type(df1.iloc[i]['Lyrics']))\n",
    "  if predict([df1.iloc[i]['Lyrics']])==1:\n",
    "    #print(\"Offensive word found\")\n",
    "    count=count+1\n",
    "    try:\n",
    "      df1=df1.drop([i])\n",
    "      actual_count=actual_count+1\n",
    "    except:\n",
    "      pass\n",
    "  else:\n",
    "    pass\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gl9E4OAsLPFm",
    "outputId": "674bd9b4-6779-439c-c2a5-2c8d00e5d362"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rock_sub = df1.sample(n=500)\n",
    "\n",
    "rock_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQqnCeFgsMaG",
    "outputId": "686a3207-3779-4699-bd50-9a65111264a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49584     (M\\nJagger/K\\nRichards)\\nSometime I wonder why...\n",
       "101402    Music by Elton John\\nLyrics by Bernie Taupin\\n...\n",
       "236804    Here we go again, wasting each other's time\\nT...\n",
       "45705     Avoir, Adieu, Goodnight\\nI’m too wrong, to get...\n",
       "59562     Her voice cut like a razor\\nA rose petal with ...\n",
       "                                ...                        \n",
       "16347     The wild boys are calling on their way back fr...\n",
       "1180      I could stay awake just to hear you breathing\\...\n",
       "262853    \\noh\\none last kiss\\noh baby one last kiss\\nit...\n",
       "44165     Don't get me started on politicians\\nAnd the l...\n",
       "21311     Too alarming now to talk about\\nTake your pict...\n",
       "Name: Lyrics, Length: 500, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rock_lyrics = rock_sub['Lyrics'].astype('str')\n",
    "\n",
    "rock_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "Z9ixFWYds9LY",
    "outputId": "476d3fd5-de74-4117-fa71-ced2b2291993"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"(M\\nJagger/K\\nRichards)\\nSometime I wonder why you do these things to me\\nSometime I worry girl that you ain't in love with me\\nSometime I stay out late, yeah I'm having fun\\nYes, I guess you know by now you ain't the only one\\nBaby, sweet things that you promised me babe\\nSeemed to go up in smoke\\nYeah, vanish like a dream\\nI wonder why you do these things to me\\nCause I'm worried\\nI just can't seem to find my way, baby\\nOoh, the nights I spent just waiting on the sun\\nJust like your burned out cigarette\\nYou threw away my love\\nWhy did you do that baby\\nI wonder why, why you do these things to me\\nI'm worried\\nLord, I'll find out anyway\\nSure going to find myself a girl someday\\nTill then I'm worried\\nYeah, I just can't seem to find my way\\nYeah, I'm a hard working man\\nWhen did I ever do you wrong?\\nYeah, I get all my money baby\\nBring it, bring it all home\\nYeah, I'm telling the truth\\nSweet things, sweet things that you promised me\\nWell I'm worried, I just can't seem to find my way, baby\\nI'm worried about you\\nI'm worried about you\\nTell you something now\\nWorried 'bout you, child\\nWorried 'bout you, woman\\nYeah, I'm worried\\nLord, I'll find out anyway\\nSure as Hell I'm going to find that girl someday\\nTill then I'm worried\\nLord, I just can't seem to find my way.\""
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocklist = list(rock_lyrics)\n",
    "\n",
    "rocklist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "6eW4aWf6uKc4",
    "outputId": "871bf534-2249-4808-edc3-e0e958f5c1bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'m jaggerk richards sometime i wonder why you do these things to me sometime i worry girl that you aint in love with me sometime i stay out late yeah im having fun yes i guess you know by now you aint the only one baby sweet things that you promised me babe seemed to go up in smoke yeah vanish like a dream i wonder why you do these things to me cause im worried i just cant seem to find my way baby ooh the nights i spent just waiting on the sun just like your burned out cigarette you threw away my love why did you do that baby i wonder why why you do these things to me im worried lord ill find out anyway sure going to find myself a girl someday till then im worried yeah i just cant seem to find my way yeah im a hard working man when did i ever do you wrong yeah i get all my money baby bring it bring it all home yeah im telling the truth sweet things sweet things that you promised me well im worried i just cant seem to find my way baby im worried about you im worried about you tell you something now worried bout you child worried bout you woman yeah im worried lord ill find out anyway sure as hell im going to find that girl someday till then im worried lord i just cant seem to find my way'"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(i for i in txt if i not in string.punctuation).lower()\n",
    "    txt = txt.replace(\"\\n\",\" \")\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in rocklist]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3Ho8_VquN9A",
    "outputId": "84724c35-40c9-4586-a7c0-fc688617b04d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A8D-mxYJNtK"
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etchBXsg2kwN"
   },
   "source": [
    "Input Sequences\n",
    "\n",
    "`Input sequence is the numerical representation of how our words are arranged` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFd3pxfP16FW"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yQuC7vz1_1g"
   },
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMEXTaTbBRFK",
    "outputId": "1d1d85cb-681c-4e5a-941c-360a63f19f14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7743"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zoo9KutZNyAg"
   },
   "source": [
    "Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_GAbMCxN2SN"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_len = max([len(x) for x in inp_sequences])\n",
    "inp_sequences = np.array(pad_sequences(inp_sequences,\n",
    "                       maxlen = max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeUpTCrnOfpL",
    "outputId": "27f830ab-4d55-4460-d204-3996168a6161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94361, 736)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,   28,    6],\n",
       "       [   0,    0,    0, ...,   28,    6, 1385],\n",
       "       [   0,    0,    0, ...,    6, 1385,  290],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  161,    6,  358],\n",
       "       [   0,    0,    0, ...,    6,  358,   16],\n",
       "       [   0,    0,    0, ...,  358,   16,    7]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inp_sequences.shape)\n",
    "inp_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqcyeXKXPgRY"
   },
   "source": [
    "Predictions and Labels\n",
    "\n",
    "we will use our input sequence and use the last word of all sequences as labels for all previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpBp8YVNP2KM"
   },
   "outputs": [],
   "source": [
    "predictors, label = inp_sequences[:,:-1],inp_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XxRgrjkVDJk"
   },
   "outputs": [],
   "source": [
    "import keras.utils as ku\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GA0woWt2T1ca",
    "outputId": "cadbd2fc-9e00-46ef-8cfa-f1877268870f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94361, 7743)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iWE-Wiw-qds"
   },
   "source": [
    "### Best NN Language Model for Rock Lyrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "waSdW_ffCG2H",
    "outputId": "e0b1f8f3-69ce-4b8e-ff1a-2d0fff6d8ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1X4QnYP7JtHFE1PLxPO-Ci8Gr3ndhiHc2 into ./model/rock01.h5... Done.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 735, 50)           387150    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3871)              390971    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7743)              29980896  \n",
      "=================================================================\n",
      "Total params: 30,819,417\n",
      "Trainable params: 30,819,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1X4QnYP7JtHFE1PLxPO-Ci8Gr3ndhiHc2', dest_path='./model/rock01.h5')\n",
    "\n",
    "rock_generator = tf.keras.models.load_model('./model/rock01.h5')\n",
    "\n",
    "rock_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzLZd6nqCeo8"
   },
   "outputs": [],
   "source": [
    "def rock_lyrics(seed_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')\n",
    "        \n",
    "        predicted = rock_generator.predict_classes(token_list, verbose=0)\n",
    "\n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtPSYiJHC6nF",
    "outputId": "830768af-f03f-492e-abc2-70874cfe7c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 735) for input KerasTensor(type_spec=TensorSpec(shape=(None, 735), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 715).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at free me you there ah a thought hand preciso a planks and can i over a that two to years gash making antes spell river beautiful bitter seja cheque be explanation’s nowhere beautiful and is was of showing you here answer oh it um gonna but up a quando this\n"
     ]
    }
   ],
   "source": [
    "rock_lyrics(\"look at\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdsXo8WRDBnN",
    "outputId": "c55e22ba-b796-4c08-c8cb-38e75ab0789d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at free me you there ah a thought hand preciso a planks and can i over a that two to years gash making antes spell river beautiful bitter seja cheque be explanation’s nowhere beautiful and is was of showing you here answer oh it um gonna but up a quando this oh water and hand colar and born my there ah its my there get into a that it chorus it eyes tonight you no call can i que me with you find shes me with you find shes me with you find shes me with you find shes me with\n"
     ]
    }
   ],
   "source": [
    "rock_lyrics(\"look at\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGhzYqogDOOC",
    "outputId": "54e0047a-fa00-4a81-961c-8836d582406b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't know clear clear the his you lugar the his and the turning and the rocking and the jukebox of on place friend on changes the drugstore isso of to mommys some in it dogs stirred appear not braved me and in to falam eternity im espacial now put evrything neurotic town\n"
     ]
    }
   ],
   "source": [
    "rock_lyrics(\"i don't know\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLaLueSsDgmC"
   },
   "source": [
    "### Statistical Model for Rock Lyrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Da8HczRMHCE8",
    "outputId": "36b08ad4-a9d3-44bd-a241-a7144fd04d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3613\n",
      "302\n",
      "534\n",
      "369\n",
      "0.03706857635326466\n",
      "0.003098452825542742\n",
      "0.005478721221323922\n",
      "0.003785857922600238\n",
      "13\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(ng_model.counts['the'])\n",
    "print(ng_model.counts['are'])\n",
    "print(ng_model.counts['love'])\n",
    "print(ng_model.counts['oh'])\n",
    "\n",
    "print(ng_model.score('the'))\n",
    "print(ng_model.score('are'))\n",
    "print(ng_model.score('love'))\n",
    "print(ng_model.score('oh'))\n",
    "\n",
    "print(ng_model.counts[['they']]['are'])\n",
    "print(ng_model.counts[['are']]['they'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpmsopDtHCE9",
    "outputId": "5ff580c0-103e-4455-deb9-836f2f010994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you who broke my heart some people do not mess with mister inbetween no dont\n"
     ]
    }
   ],
   "source": [
    "text_list = ng_model.generate(15, random_seed=2)\n",
    "print(' '.join(word for word in text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgASq2A6HCE9",
    "outputId": "c413bb74-9840-4641-ba6f-97357151877e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you who broke my heart some people do not mess with mister inbetween no dont you worry baby im buying if youre my daughter youre my love ill get it back better late than never i saw you saying that you take my woman where she wants to do it\n"
     ]
    }
   ],
   "source": [
    "text_list = ng_model.generate(50, random_seed=2)\n",
    "print(' '.join(word for word in text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fhog6BdHCE-",
    "outputId": "643a16a5-c248-4765-f6fa-8536539cc79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy who was hanging his head low more trophies and ideas to follow footprints in the darkest place that cow wrote that im doing it all a big joke whatever it is like porcelain yeah </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    }
   ],
   "source": [
    "text_list = ng_model.generate(50, random_seed=1)\n",
    "print(' '.join(word for word in text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34ha1k7qHmCA"
   },
   "source": [
    "## Pop Lyrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffb1NrBjqKOB",
    "outputId": "a8897fd2-922e-4bea-8f3b-fb1f93ae618a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_sub = df1.head(500)\n",
    "\n",
    "pop_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhkC-GngcA2C"
   },
   "outputs": [],
   "source": [
    "pop_lyrics = pop_sub['Lyrics'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "4lkPmXRvixhC",
    "outputId": "8d3cf4bc-3222-469e-d116-77e3d22e91d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"lives in a little lonely town\\nno one's around except for the drinking\\nnobody even gets around\\nbut those who leave the township sinking\\nmay you rot in heaven\\ngotta be home by seven\\nand the fields burns away\\nthe sky breathes it in\\nso why sit and wait\\nfor the new world to begin\\ni'm comin' out across your border\\nwith new orders for you to take\\ni'd really like to take out yuor daughter\\ndown by the water down by the lake\\nwhen cold water's on her skin\\ni can feel how long it's been\\nand the neighbours will all be there\\nand no one will know what to wear\\nso why sit and wait\\nfor the new world to begin\\ni got a lot i gotta do\\njust to get through the end of the day\\nit hardly ever happens\\nbut i go to sleep the same anyway\\nand you can't believe in yourself\\nyou can't believe in anyone else\\nso why sit and wait\\nfor the new world to begin.\""
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poplist = list(pop_lyrics)\n",
    "\n",
    "poplist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iusQLUy8fcHT"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(i for i in txt if i not in string.punctuation).lower()\n",
    "    txt = txt.replace(\"\\n\",\" \")\n",
    "    return txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "t2Itl5JHiPgq",
    "outputId": "bf5c6e28-4040-42bf-fda9-2fbfc16311f0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'lives in a little lonely town no ones around except for the drinking nobody even gets around but those who leave the township sinking may you rot in heaven gotta be home by seven and the fields burns away the sky breathes it in so why sit and wait for the new world to begin im comin out across your border with new orders for you to take id really like to take out yuor daughter down by the water down by the lake when cold waters on her skin i can feel how long its been and the neighbours will all be there and no one will know what to wear so why sit and wait for the new world to begin i got a lot i gotta do just to get through the end of the day it hardly ever happens but i go to sleep the same anyway and you cant believe in yourself you cant believe in anyone else so why sit and wait for the new world to begin'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in poplist]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qru7CmVjkfXk",
    "outputId": "aba7ab7a-8c11-4419-d4be-c2344338b8c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTfYGycHoSbc"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Gt-BTtRonDK"
   },
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8I38CC3posWS",
    "outputId": "cd6d3bbd-4df2-411b-dad8-adcde43c6f11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11939"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgi2SDghq4Lm",
    "outputId": "4deab1e2-d536-4260-ba52-89a8661b00b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_len = 500\n",
    "print(max_sequence_len)\n",
    "inp_sequences = np.array(pad_sequences(inp_sequences,\n",
    "                       maxlen = max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uP1tne4PrTpc"
   },
   "outputs": [],
   "source": [
    "predictors, label = inp_sequences[:,:-1],inp_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj9hJ6wArXDm"
   },
   "outputs": [],
   "source": [
    "import keras.utils as ku\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJnuz7RirZke",
    "outputId": "e017f1dc-4792-423c-8400-d7d2a01ee347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123751, 6416)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6KdZ02F4UNm",
    "outputId": "b444968e-b1ae-4f38-c477-6854faf5e334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 50)           320800    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3208)              324008    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6416)              20588944  \n",
      "=================================================================\n",
      "Total params: 21,294,152\n",
      "Trainable params: 21,294,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1VlqLVdrbQ-BRR-K4_1tCvid6sTtZj6EN', dest_path='./model/pop01.h5')\n",
    "\n",
    "pop_generator = tf.keras.models.load_model('./model/pop01.h5')\n",
    "\n",
    "pop_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9X_dPhCJvRh"
   },
   "outputs": [],
   "source": [
    "def pop_lyrics(seed_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')\n",
    "        \n",
    "        predicted = pop_generator.predict_classes(token_list, verbose=0)\n",
    "\n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMcW5TRXOysq",
    "outputId": "68953871-165d-4a08-dfe9-167cdaa78681"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They look for me im a tangled puppet and i might be a mess but i sure can survive find myself awake counting sad days 1 2 3 thats too many for too cool for dawn jump a wonderful country but the man hes burning it down yea yea yea yea yea yea\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (make_lyrics(\"They look for\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SFcYP3fO0zg",
    "outputId": "165d2cdd-c02f-449c-ec07-5e9dd5acabaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you know well i met you caught up slurring cos shake fire shake machine im a live wire live wire gonna set this town on fire live wire live wire live wire im a live wire live wire im a live wire live wire im a live wire live wire im a live\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (make_lyrics(\"you know well\", 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2UkyHPKHqdW"
   },
   "source": [
    "## Metal Lyrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0g-Xg6Lh5ma",
    "outputId": "0313675a-45dd-4b35-b9ff-598a32d292cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metal_sub = df1.sample(n=500)\n",
    "metal_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raff0P5DL9bN"
   },
   "outputs": [],
   "source": [
    "metal_lyrics = metal_sub['Lyrics'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "eOzara90L__6",
    "outputId": "a2769772-3e25-4e08-b87a-915a3681fd3d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Take a chance n something star you gotta\\nAll the live long day\\nIf I had to name one thing would data\\nBe hard pressed to say\\nYou can say that I’m gushy\\nAt least you know what I think\\nI know that I’m lucky I know you’re on the brink\\nYou got it good good good you got it good so good\\nYou make up ridiculous words\\nWhich mean exactly nothing\\nI know just what you mean and that’s the funny thing\\nIt’s understood it’s understood\\nBuddy buddy buddy I know why you wander\\nThrough the sweetness on the ground\\nThe sweetness of the flowers\\nYou make your way on a cloudy day\\nYou’ve got to move\\nThe world you prove is too sweet yeah\\nWe parted much too soon I know but that’s how it goes\\nThere’s someone better out there for you and me\\nCan’t wait to get on the road and feel the shows\\nAnd be like phenomenon 1 and 2 and 3\\nI don’t mean to brag and I don’t mean to boast\\nBut I’m the guy could that give you the most\\nIt’s good good good it’s understood\\nChorus'"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metallist = list(metal_lyrics)\n",
    "\n",
    "metallist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCgj7F1SMCjL"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace(\"\\n\",\" \")\n",
    "    return txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "jqRlXdG1MILN",
    "outputId": "6398de87-4559-411d-e117-d53df5c4d73c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'take a chance n something star you gotta all the live long day if i had to name one thing would data be hard pressed to say you can say that i’m gushy at least you know what i think i know that i’m lucky i know you’re on the brink you got it good good good you got it good so good you make up ridiculous words which mean exactly nothing i know just what you mean and that’s the funny thing it’s understood it’s understood buddy buddy buddy i know why you wander through the sweetness on the ground the sweetness of the flowers you make your way on a cloudy day you’ve got to move the world you prove is too sweet yeah we parted much too soon i know but that’s how it goes there’s someone better out there for you and me can’t wait to get on the road and feel the shows and be like phenomenon 1 and 2 and 3 i don’t mean to brag and i don’t mean to boast but i’m the guy could that give you the most it’s good good good it’s understood chorus'"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in metallist]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XSpwwe4MLPS",
    "outputId": "aba7ab7a-8c11-4419-d4be-c2344338b8c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIFQG4-8MNpm"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miAhhTX8McWB"
   },
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO17eUn6MXcx"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_len = 500\n",
    "\n",
    "inp_sequences = np.array(pad_sequences(inp_sequences,\n",
    "                       maxlen = max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AXr0DE7MgE4"
   },
   "outputs": [],
   "source": [
    "predictors, label = inp_sequences[:,:-1],inp_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syNnmccJMiDQ"
   },
   "outputs": [],
   "source": [
    "import keras.utils as ku\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgF0vhdoMomB",
    "outputId": "a445e63e-7dee-45d1-ac38-34fd99badd30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 499, 50)           324950    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3249)              328149    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6499)              21121750  \n",
      "=================================================================\n",
      "Total params: 21,835,249\n",
      "Trainable params: 21,835,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1FF2m7cQSaxCzlYR1NcRCkUFvdT7Llnun', dest_path='./model/metal01.h5')\n",
    "\n",
    "metal_generator = tf.keras.models.load_model('./model/metal01.h5')\n",
    "\n",
    "metal_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iEPZNklKmRN"
   },
   "outputs": [],
   "source": [
    "def make_lyrics(seed_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')\n",
    "        predicted = metal_generator.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQ54_I4FLcq2",
    "outputId": "5d9795e9-ff09-4cbe-a734-b941c4adaa47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light cool the same sweatin in the rain so watch out there's a division in our life how do we take back what's been done what's been said oh my how do we take back 'cuz no one wins the wheel of them what you got so rude boy actin' coy you got nuts liek almond joy i tell ya this is a tribute i'm vexing many mofos but i'm wishing you come a day i didn't sit and mean so much can't hear about it stand up stand up stand up and face up and shakin’ it up now we just go yo dope rhymes might get crazy but they're not already there is not how it has it been since you let me in i need your heart and tears you laugh like then you think that i've done you don't mind yourself and if you dare nothing then waitings what i'll do what you mean screaming silent fears nothing don't try to be a prick to be a a a a a a a a little bit of what i'm a i will not attempt to try to be free but i reserve the right to be stoned i\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (make_lyrics(\"light cool\", 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgZ0c7GkP5SZ"
   },
   "source": [
    "## Statistical model for Lyrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-jYfaQ_gV7i",
    "outputId": "299a3d5d-1b0a-4e6f-f87f-60a3d6802a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 9.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.0.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.41.1)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434673 sha256=c0084329af7008bbff1db00f3139f25918256a5ecde9a23a8fe91b0cf89e58ff\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk==3.5\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk import word_tokenize\n",
    "# we need to download a special component that is used by the tokenizer below -- don't worry about it. \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIJdYvYJgqgR"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "qnfwJbREh7Fu",
    "outputId": "f0df7167-5db1-49b5-e6a4-535024f23e0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>world so cold</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>It starts with pain, followed by hate\\nFueled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>broken</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Freedom!\\nAlone again again alone\\nPatiently w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>3 leaf loser</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>Biting the hand that feeds you, lying to the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>anthem for the underdog</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>You say you know just who I am\\nBut you can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 stones</td>\n",
       "      <td>adrenaline</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>My heart is beating faster can't control these...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290178</th>\n",
       "      <td>bobby womack</td>\n",
       "      <td>i wish he didn t trust me so much</td>\n",
       "      <td>R&amp;B</td>\n",
       "      <td>en</td>\n",
       "      <td>I'm the best friend he's got I'd give him the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290179</th>\n",
       "      <td>bad boys blue</td>\n",
       "      <td>i totally miss you</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Bad Boys Blue \"I Totally Miss You\" I did you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290180</th>\n",
       "      <td>celine dion</td>\n",
       "      <td>sorry for love</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Forgive me for the things That I never said to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290181</th>\n",
       "      <td>dan bern</td>\n",
       "      <td>cure for aids</td>\n",
       "      <td>Indie</td>\n",
       "      <td>en</td>\n",
       "      <td>The day they found a cure for AIDS The day the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290182</th>\n",
       "      <td>crawdad republic</td>\n",
       "      <td>iceberg meadows</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>Fourth of July has come, it's custom that we g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250197 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Artist  ...                                             Lyrics\n",
       "0              12 stones  ...  It starts with pain, followed by hate\\nFueled ...\n",
       "1              12 stones  ...  Freedom!\\nAlone again again alone\\nPatiently w...\n",
       "2              12 stones  ...  Biting the hand that feeds you, lying to the v...\n",
       "3              12 stones  ...  You say you know just who I am\\nBut you can't ...\n",
       "4              12 stones  ...  My heart is beating faster can't control these...\n",
       "...                  ...  ...                                                ...\n",
       "290178      bobby womack  ...  I'm the best friend he's got I'd give him the ...\n",
       "290179     bad boys blue  ...  Bad Boys Blue \"I Totally Miss You\" I did you w...\n",
       "290180       celine dion  ...  Forgive me for the things That I never said to...\n",
       "290181          dan bern  ...  The day they found a cure for AIDS The day the...\n",
       "290182  crawdad republic  ...  Fourth of July has come, it's custom that we g...\n",
       "\n",
       "[250197 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_data = data[data['Language']=='en']\n",
    "en_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ze7h54NYh9P9",
    "outputId": "53bb7b69-72e7-4170-d70b-03eee17e5b5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|\\n@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]') # removing any symbol that is NOT number, letter, space, '#', '+', or '_'.\n",
    "STOPWORDS = set(stopwords.words('english')) # only dealing with English Lyrics, so set stopwords to 'english'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
    "    return text\n",
    "\n",
    "en_data['Lyrics'] = en_data['Lyrics'].apply(clean_text)\n",
    "en_data['Lyrics'] = en_data['Lyrics'].str.replace('\\d+', '') # replacing one or more digits by nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_KfXisqkBIq"
   },
   "outputs": [],
   "source": [
    "lyrics = en_data['Lyrics'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCfMAdtGRLzB"
   },
   "source": [
    "taking 30% of total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EI5THhTinKJ-"
   },
   "outputs": [],
   "source": [
    "extract = 0.3\n",
    "lyrics = lyrics[:int(extract*len(lyrics))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_R0v__xnUu4",
    "outputId": "40a13301-4f18-4e0b-c3a6-2fe2ee3a6723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['starts', 'pain', 'followed', 'hate', 'fueled', 'endless', 'questions', 'one', 'answer', 'stain', 'covers', 'heart', 'tears', 'apart', 'like', 'sleeping', 'cancer', 'dont', 'believe', 'men', 'born', 'killers', 'dont', 'believe', 'world', 'saved', 'get', 'start', 'innocent', 'child', 'thorn', 'heart', 'kind', 'world', 'live', 'love', 'divided', 'hate', 'loosing', 'control', 'feelings', 'must', 'dreaming', 'life', 'away', 'world', 'cold', 'sane', 'wheres', 'shame', 'moment', 'time', 'passes', 'can', 'not', 'rewind', 'whos', 'blame', 'start', 'cure', 'sickness', 'heart', 'dont', 'believe', 'men', 'born', 'killers', 'dont', 'believe', 'world', 'cant', 'saved', 'get', 'start', 'innocent', 'child', 'thorn', 'heart', 'kind', 'world', 'live', 'love', 'divided', 'hate', 'selling', 'soul', 'reason', 'must', 'dreaming', 'life', 'away', 'world', 'cold', 'world', 'cold', 'theres', 'sickness', 'inside', 'wants', 'escape', 'feeling', 'get', 'cant', 'find', 'way', 'many', 'times', 'must', 'fall', 'knees', 'never', 'never', 'never', 'never', 'never', 'starts', 'pain', 'followed', 'hate', 'dont', 'believe', 'men', 'born', 'killers', 'dont', 'believe', 'world', 'cant', 'saved', 'kind', 'world', 'live', 'love', 'divided', 'hate', 'losing', 'control', 'feeling', 'dreaming', 'life', 'away', 'kind', 'world', 'live', 'love', 'divided', 'hate', 'selling', 'soul', 'reason', 'must', 'dreaming', 'life', 'away', 'world', 'cold', 'world', 'cold']\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for s in lyrics:\n",
    "    texts.append(word_tokenize(s))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBJRzulsnw3h",
    "outputId": "4d68861c-81e9-4177-e05f-cf7f2e2fd06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('starts',), ('pain',), ('followed',), ('hate',), ('fueled',), ('endless',), ('questions',), ('one',), ('answer',), ('stain',), ('covers',), ('heart',), ('tears',), ('apart',), ('like',), ('sleeping',), ('cancer',), ('dont',), ('believe',), ('men',), ('born',), ('killers',), ('dont',), ('believe',), ('world',), ('saved',), ('get',), ('start',), ('innocent',), ('child',), ('thorn',), ('heart',), ('kind',), ('world',), ('live',), ('love',), ('divided',), ('hate',), ('loosing',), ('control',), ('feelings',), ('must',), ('dreaming',), ('life',), ('away',), ('world',), ('cold',), ('sane',), ('wheres',), ('shame',), ('moment',), ('time',), ('passes',), ('can',), ('not',), ('rewind',), ('whos',), ('blame',), ('start',), ('cure',), ('sickness',), ('heart',), ('dont',), ('believe',), ('men',), ('born',), ('killers',), ('dont',), ('believe',), ('world',), ('cant',), ('saved',), ('get',), ('start',), ('innocent',), ('child',), ('thorn',), ('heart',), ('kind',), ('world',), ('live',), ('love',), ('divided',), ('hate',), ('selling',), ('soul',), ('reason',), ('must',), ('dreaming',), ('life',), ('away',), ('world',), ('cold',), ('world',), ('cold',), ('theres',), ('sickness',), ('inside',), ('wants',), ('escape',), ('feeling',), ('get',), ('cant',), ('find',), ('way',), ('many',), ('times',), ('must',), ('fall',), ('knees',), ('never',), ('never',), ('never',), ('never',), ('never',), ('starts',), ('pain',), ('followed',), ('hate',), ('dont',), ('believe',), ('men',), ('born',), ('killers',), ('dont',), ('believe',), ('world',), ('cant',), ('saved',), ('kind',), ('world',), ('live',), ('love',), ('divided',), ('hate',), ('losing',), ('control',), ('feeling',), ('dreaming',), ('life',), ('away',), ('kind',), ('world',), ('live',), ('love',), ('divided',), ('hate',), ('selling',), ('soul',), ('reason',), ('must',), ('dreaming',), ('life',), ('away',), ('world',), ('cold',), ('world',), ('cold',)]\n",
      "[('starts', 'pain'), ('pain', 'followed'), ('followed', 'hate'), ('hate', 'fueled'), ('fueled', 'endless'), ('endless', 'questions'), ('questions', 'one'), ('one', 'answer'), ('answer', 'stain'), ('stain', 'covers'), ('covers', 'heart'), ('heart', 'tears'), ('tears', 'apart'), ('apart', 'like'), ('like', 'sleeping'), ('sleeping', 'cancer'), ('cancer', 'dont'), ('dont', 'believe'), ('believe', 'men'), ('men', 'born'), ('born', 'killers'), ('killers', 'dont'), ('dont', 'believe'), ('believe', 'world'), ('world', 'saved'), ('saved', 'get'), ('get', 'start'), ('start', 'innocent'), ('innocent', 'child'), ('child', 'thorn'), ('thorn', 'heart'), ('heart', 'kind'), ('kind', 'world'), ('world', 'live'), ('live', 'love'), ('love', 'divided'), ('divided', 'hate'), ('hate', 'loosing'), ('loosing', 'control'), ('control', 'feelings'), ('feelings', 'must'), ('must', 'dreaming'), ('dreaming', 'life'), ('life', 'away'), ('away', 'world'), ('world', 'cold'), ('cold', 'sane'), ('sane', 'wheres'), ('wheres', 'shame'), ('shame', 'moment'), ('moment', 'time'), ('time', 'passes'), ('passes', 'can'), ('can', 'not'), ('not', 'rewind'), ('rewind', 'whos'), ('whos', 'blame'), ('blame', 'start'), ('start', 'cure'), ('cure', 'sickness'), ('sickness', 'heart'), ('heart', 'dont'), ('dont', 'believe'), ('believe', 'men'), ('men', 'born'), ('born', 'killers'), ('killers', 'dont'), ('dont', 'believe'), ('believe', 'world'), ('world', 'cant'), ('cant', 'saved'), ('saved', 'get'), ('get', 'start'), ('start', 'innocent'), ('innocent', 'child'), ('child', 'thorn'), ('thorn', 'heart'), ('heart', 'kind'), ('kind', 'world'), ('world', 'live'), ('live', 'love'), ('love', 'divided'), ('divided', 'hate'), ('hate', 'selling'), ('selling', 'soul'), ('soul', 'reason'), ('reason', 'must'), ('must', 'dreaming'), ('dreaming', 'life'), ('life', 'away'), ('away', 'world'), ('world', 'cold'), ('cold', 'world'), ('world', 'cold'), ('cold', 'theres'), ('theres', 'sickness'), ('sickness', 'inside'), ('inside', 'wants'), ('wants', 'escape'), ('escape', 'feeling'), ('feeling', 'get'), ('get', 'cant'), ('cant', 'find'), ('find', 'way'), ('way', 'many'), ('many', 'times'), ('times', 'must'), ('must', 'fall'), ('fall', 'knees'), ('knees', 'never'), ('never', 'never'), ('never', 'never'), ('never', 'never'), ('never', 'never'), ('never', 'starts'), ('starts', 'pain'), ('pain', 'followed'), ('followed', 'hate'), ('hate', 'dont'), ('dont', 'believe'), ('believe', 'men'), ('men', 'born'), ('born', 'killers'), ('killers', 'dont'), ('dont', 'believe'), ('believe', 'world'), ('world', 'cant'), ('cant', 'saved'), ('saved', 'kind'), ('kind', 'world'), ('world', 'live'), ('live', 'love'), ('love', 'divided'), ('divided', 'hate'), ('hate', 'losing'), ('losing', 'control'), ('control', 'feeling'), ('feeling', 'dreaming'), ('dreaming', 'life'), ('life', 'away'), ('away', 'kind'), ('kind', 'world'), ('world', 'live'), ('live', 'love'), ('love', 'divided'), ('divided', 'hate'), ('hate', 'selling'), ('selling', 'soul'), ('soul', 'reason'), ('reason', 'must'), ('must', 'dreaming'), ('dreaming', 'life'), ('life', 'away'), ('away', 'world'), ('world', 'cold'), ('cold', 'world'), ('world', 'cold')]\n",
      "[('starts', 'pain', 'followed'), ('pain', 'followed', 'hate'), ('followed', 'hate', 'fueled'), ('hate', 'fueled', 'endless'), ('fueled', 'endless', 'questions'), ('endless', 'questions', 'one'), ('questions', 'one', 'answer'), ('one', 'answer', 'stain'), ('answer', 'stain', 'covers'), ('stain', 'covers', 'heart'), ('covers', 'heart', 'tears'), ('heart', 'tears', 'apart'), ('tears', 'apart', 'like'), ('apart', 'like', 'sleeping'), ('like', 'sleeping', 'cancer'), ('sleeping', 'cancer', 'dont'), ('cancer', 'dont', 'believe'), ('dont', 'believe', 'men'), ('believe', 'men', 'born'), ('men', 'born', 'killers'), ('born', 'killers', 'dont'), ('killers', 'dont', 'believe'), ('dont', 'believe', 'world'), ('believe', 'world', 'saved'), ('world', 'saved', 'get'), ('saved', 'get', 'start'), ('get', 'start', 'innocent'), ('start', 'innocent', 'child'), ('innocent', 'child', 'thorn'), ('child', 'thorn', 'heart'), ('thorn', 'heart', 'kind'), ('heart', 'kind', 'world'), ('kind', 'world', 'live'), ('world', 'live', 'love'), ('live', 'love', 'divided'), ('love', 'divided', 'hate'), ('divided', 'hate', 'loosing'), ('hate', 'loosing', 'control'), ('loosing', 'control', 'feelings'), ('control', 'feelings', 'must'), ('feelings', 'must', 'dreaming'), ('must', 'dreaming', 'life'), ('dreaming', 'life', 'away'), ('life', 'away', 'world'), ('away', 'world', 'cold'), ('world', 'cold', 'sane'), ('cold', 'sane', 'wheres'), ('sane', 'wheres', 'shame'), ('wheres', 'shame', 'moment'), ('shame', 'moment', 'time'), ('moment', 'time', 'passes'), ('time', 'passes', 'can'), ('passes', 'can', 'not'), ('can', 'not', 'rewind'), ('not', 'rewind', 'whos'), ('rewind', 'whos', 'blame'), ('whos', 'blame', 'start'), ('blame', 'start', 'cure'), ('start', 'cure', 'sickness'), ('cure', 'sickness', 'heart'), ('sickness', 'heart', 'dont'), ('heart', 'dont', 'believe'), ('dont', 'believe', 'men'), ('believe', 'men', 'born'), ('men', 'born', 'killers'), ('born', 'killers', 'dont'), ('killers', 'dont', 'believe'), ('dont', 'believe', 'world'), ('believe', 'world', 'cant'), ('world', 'cant', 'saved'), ('cant', 'saved', 'get'), ('saved', 'get', 'start'), ('get', 'start', 'innocent'), ('start', 'innocent', 'child'), ('innocent', 'child', 'thorn'), ('child', 'thorn', 'heart'), ('thorn', 'heart', 'kind'), ('heart', 'kind', 'world'), ('kind', 'world', 'live'), ('world', 'live', 'love'), ('live', 'love', 'divided'), ('love', 'divided', 'hate'), ('divided', 'hate', 'selling'), ('hate', 'selling', 'soul'), ('selling', 'soul', 'reason'), ('soul', 'reason', 'must'), ('reason', 'must', 'dreaming'), ('must', 'dreaming', 'life'), ('dreaming', 'life', 'away'), ('life', 'away', 'world'), ('away', 'world', 'cold'), ('world', 'cold', 'world'), ('cold', 'world', 'cold'), ('world', 'cold', 'theres'), ('cold', 'theres', 'sickness'), ('theres', 'sickness', 'inside'), ('sickness', 'inside', 'wants'), ('inside', 'wants', 'escape'), ('wants', 'escape', 'feeling'), ('escape', 'feeling', 'get'), ('feeling', 'get', 'cant'), ('get', 'cant', 'find'), ('cant', 'find', 'way'), ('find', 'way', 'many'), ('way', 'many', 'times'), ('many', 'times', 'must'), ('times', 'must', 'fall'), ('must', 'fall', 'knees'), ('fall', 'knees', 'never'), ('knees', 'never', 'never'), ('never', 'never', 'never'), ('never', 'never', 'never'), ('never', 'never', 'never'), ('never', 'never', 'starts'), ('never', 'starts', 'pain'), ('starts', 'pain', 'followed'), ('pain', 'followed', 'hate'), ('followed', 'hate', 'dont'), ('hate', 'dont', 'believe'), ('dont', 'believe', 'men'), ('believe', 'men', 'born'), ('men', 'born', 'killers'), ('born', 'killers', 'dont'), ('killers', 'dont', 'believe'), ('dont', 'believe', 'world'), ('believe', 'world', 'cant'), ('world', 'cant', 'saved'), ('cant', 'saved', 'kind'), ('saved', 'kind', 'world'), ('kind', 'world', 'live'), ('world', 'live', 'love'), ('live', 'love', 'divided'), ('love', 'divided', 'hate'), ('divided', 'hate', 'losing'), ('hate', 'losing', 'control'), ('losing', 'control', 'feeling'), ('control', 'feeling', 'dreaming'), ('feeling', 'dreaming', 'life'), ('dreaming', 'life', 'away'), ('life', 'away', 'kind'), ('away', 'kind', 'world'), ('kind', 'world', 'live'), ('world', 'live', 'love'), ('live', 'love', 'divided'), ('love', 'divided', 'hate'), ('divided', 'hate', 'selling'), ('hate', 'selling', 'soul'), ('selling', 'soul', 'reason'), ('soul', 'reason', 'must'), ('reason', 'must', 'dreaming'), ('must', 'dreaming', 'life'), ('dreaming', 'life', 'away'), ('life', 'away', 'world'), ('away', 'world', 'cold'), ('world', 'cold', 'world'), ('cold', 'world', 'cold')]\n",
      "[('starts', 'pain', 'followed', 'hate'), ('pain', 'followed', 'hate', 'fueled'), ('followed', 'hate', 'fueled', 'endless'), ('hate', 'fueled', 'endless', 'questions'), ('fueled', 'endless', 'questions', 'one'), ('endless', 'questions', 'one', 'answer'), ('questions', 'one', 'answer', 'stain'), ('one', 'answer', 'stain', 'covers'), ('answer', 'stain', 'covers', 'heart'), ('stain', 'covers', 'heart', 'tears'), ('covers', 'heart', 'tears', 'apart'), ('heart', 'tears', 'apart', 'like'), ('tears', 'apart', 'like', 'sleeping'), ('apart', 'like', 'sleeping', 'cancer'), ('like', 'sleeping', 'cancer', 'dont'), ('sleeping', 'cancer', 'dont', 'believe'), ('cancer', 'dont', 'believe', 'men'), ('dont', 'believe', 'men', 'born'), ('believe', 'men', 'born', 'killers'), ('men', 'born', 'killers', 'dont'), ('born', 'killers', 'dont', 'believe'), ('killers', 'dont', 'believe', 'world'), ('dont', 'believe', 'world', 'saved'), ('believe', 'world', 'saved', 'get'), ('world', 'saved', 'get', 'start'), ('saved', 'get', 'start', 'innocent'), ('get', 'start', 'innocent', 'child'), ('start', 'innocent', 'child', 'thorn'), ('innocent', 'child', 'thorn', 'heart'), ('child', 'thorn', 'heart', 'kind'), ('thorn', 'heart', 'kind', 'world'), ('heart', 'kind', 'world', 'live'), ('kind', 'world', 'live', 'love'), ('world', 'live', 'love', 'divided'), ('live', 'love', 'divided', 'hate'), ('love', 'divided', 'hate', 'loosing'), ('divided', 'hate', 'loosing', 'control'), ('hate', 'loosing', 'control', 'feelings'), ('loosing', 'control', 'feelings', 'must'), ('control', 'feelings', 'must', 'dreaming'), ('feelings', 'must', 'dreaming', 'life'), ('must', 'dreaming', 'life', 'away'), ('dreaming', 'life', 'away', 'world'), ('life', 'away', 'world', 'cold'), ('away', 'world', 'cold', 'sane'), ('world', 'cold', 'sane', 'wheres'), ('cold', 'sane', 'wheres', 'shame'), ('sane', 'wheres', 'shame', 'moment'), ('wheres', 'shame', 'moment', 'time'), ('shame', 'moment', 'time', 'passes'), ('moment', 'time', 'passes', 'can'), ('time', 'passes', 'can', 'not'), ('passes', 'can', 'not', 'rewind'), ('can', 'not', 'rewind', 'whos'), ('not', 'rewind', 'whos', 'blame'), ('rewind', 'whos', 'blame', 'start'), ('whos', 'blame', 'start', 'cure'), ('blame', 'start', 'cure', 'sickness'), ('start', 'cure', 'sickness', 'heart'), ('cure', 'sickness', 'heart', 'dont'), ('sickness', 'heart', 'dont', 'believe'), ('heart', 'dont', 'believe', 'men'), ('dont', 'believe', 'men', 'born'), ('believe', 'men', 'born', 'killers'), ('men', 'born', 'killers', 'dont'), ('born', 'killers', 'dont', 'believe'), ('killers', 'dont', 'believe', 'world'), ('dont', 'believe', 'world', 'cant'), ('believe', 'world', 'cant', 'saved'), ('world', 'cant', 'saved', 'get'), ('cant', 'saved', 'get', 'start'), ('saved', 'get', 'start', 'innocent'), ('get', 'start', 'innocent', 'child'), ('start', 'innocent', 'child', 'thorn'), ('innocent', 'child', 'thorn', 'heart'), ('child', 'thorn', 'heart', 'kind'), ('thorn', 'heart', 'kind', 'world'), ('heart', 'kind', 'world', 'live'), ('kind', 'world', 'live', 'love'), ('world', 'live', 'love', 'divided'), ('live', 'love', 'divided', 'hate'), ('love', 'divided', 'hate', 'selling'), ('divided', 'hate', 'selling', 'soul'), ('hate', 'selling', 'soul', 'reason'), ('selling', 'soul', 'reason', 'must'), ('soul', 'reason', 'must', 'dreaming'), ('reason', 'must', 'dreaming', 'life'), ('must', 'dreaming', 'life', 'away'), ('dreaming', 'life', 'away', 'world'), ('life', 'away', 'world', 'cold'), ('away', 'world', 'cold', 'world'), ('world', 'cold', 'world', 'cold'), ('cold', 'world', 'cold', 'theres'), ('world', 'cold', 'theres', 'sickness'), ('cold', 'theres', 'sickness', 'inside'), ('theres', 'sickness', 'inside', 'wants'), ('sickness', 'inside', 'wants', 'escape'), ('inside', 'wants', 'escape', 'feeling'), ('wants', 'escape', 'feeling', 'get'), ('escape', 'feeling', 'get', 'cant'), ('feeling', 'get', 'cant', 'find'), ('get', 'cant', 'find', 'way'), ('cant', 'find', 'way', 'many'), ('find', 'way', 'many', 'times'), ('way', 'many', 'times', 'must'), ('many', 'times', 'must', 'fall'), ('times', 'must', 'fall', 'knees'), ('must', 'fall', 'knees', 'never'), ('fall', 'knees', 'never', 'never'), ('knees', 'never', 'never', 'never'), ('never', 'never', 'never', 'never'), ('never', 'never', 'never', 'never'), ('never', 'never', 'never', 'starts'), ('never', 'never', 'starts', 'pain'), ('never', 'starts', 'pain', 'followed'), ('starts', 'pain', 'followed', 'hate'), ('pain', 'followed', 'hate', 'dont'), ('followed', 'hate', 'dont', 'believe'), ('hate', 'dont', 'believe', 'men'), ('dont', 'believe', 'men', 'born'), ('believe', 'men', 'born', 'killers'), ('men', 'born', 'killers', 'dont'), ('born', 'killers', 'dont', 'believe'), ('killers', 'dont', 'believe', 'world'), ('dont', 'believe', 'world', 'cant'), ('believe', 'world', 'cant', 'saved'), ('world', 'cant', 'saved', 'kind'), ('cant', 'saved', 'kind', 'world'), ('saved', 'kind', 'world', 'live'), ('kind', 'world', 'live', 'love'), ('world', 'live', 'love', 'divided'), ('live', 'love', 'divided', 'hate'), ('love', 'divided', 'hate', 'losing'), ('divided', 'hate', 'losing', 'control'), ('hate', 'losing', 'control', 'feeling'), ('losing', 'control', 'feeling', 'dreaming'), ('control', 'feeling', 'dreaming', 'life'), ('feeling', 'dreaming', 'life', 'away'), ('dreaming', 'life', 'away', 'kind'), ('life', 'away', 'kind', 'world'), ('away', 'kind', 'world', 'live'), ('kind', 'world', 'live', 'love'), ('world', 'live', 'love', 'divided'), ('live', 'love', 'divided', 'hate'), ('love', 'divided', 'hate', 'selling'), ('divided', 'hate', 'selling', 'soul'), ('hate', 'selling', 'soul', 'reason'), ('selling', 'soul', 'reason', 'must'), ('soul', 'reason', 'must', 'dreaming'), ('reason', 'must', 'dreaming', 'life'), ('must', 'dreaming', 'life', 'away'), ('dreaming', 'life', 'away', 'world'), ('life', 'away', 'world', 'cold'), ('away', 'world', 'cold', 'world'), ('world', 'cold', 'world', 'cold')]\n",
      "[('starts', 'pain', 'followed', 'hate', 'fueled'), ('pain', 'followed', 'hate', 'fueled', 'endless'), ('followed', 'hate', 'fueled', 'endless', 'questions'), ('hate', 'fueled', 'endless', 'questions', 'one'), ('fueled', 'endless', 'questions', 'one', 'answer'), ('endless', 'questions', 'one', 'answer', 'stain'), ('questions', 'one', 'answer', 'stain', 'covers'), ('one', 'answer', 'stain', 'covers', 'heart'), ('answer', 'stain', 'covers', 'heart', 'tears'), ('stain', 'covers', 'heart', 'tears', 'apart'), ('covers', 'heart', 'tears', 'apart', 'like'), ('heart', 'tears', 'apart', 'like', 'sleeping'), ('tears', 'apart', 'like', 'sleeping', 'cancer'), ('apart', 'like', 'sleeping', 'cancer', 'dont'), ('like', 'sleeping', 'cancer', 'dont', 'believe'), ('sleeping', 'cancer', 'dont', 'believe', 'men'), ('cancer', 'dont', 'believe', 'men', 'born'), ('dont', 'believe', 'men', 'born', 'killers'), ('believe', 'men', 'born', 'killers', 'dont'), ('men', 'born', 'killers', 'dont', 'believe'), ('born', 'killers', 'dont', 'believe', 'world'), ('killers', 'dont', 'believe', 'world', 'saved'), ('dont', 'believe', 'world', 'saved', 'get'), ('believe', 'world', 'saved', 'get', 'start'), ('world', 'saved', 'get', 'start', 'innocent'), ('saved', 'get', 'start', 'innocent', 'child'), ('get', 'start', 'innocent', 'child', 'thorn'), ('start', 'innocent', 'child', 'thorn', 'heart'), ('innocent', 'child', 'thorn', 'heart', 'kind'), ('child', 'thorn', 'heart', 'kind', 'world'), ('thorn', 'heart', 'kind', 'world', 'live'), ('heart', 'kind', 'world', 'live', 'love'), ('kind', 'world', 'live', 'love', 'divided'), ('world', 'live', 'love', 'divided', 'hate'), ('live', 'love', 'divided', 'hate', 'loosing'), ('love', 'divided', 'hate', 'loosing', 'control'), ('divided', 'hate', 'loosing', 'control', 'feelings'), ('hate', 'loosing', 'control', 'feelings', 'must'), ('loosing', 'control', 'feelings', 'must', 'dreaming'), ('control', 'feelings', 'must', 'dreaming', 'life'), ('feelings', 'must', 'dreaming', 'life', 'away'), ('must', 'dreaming', 'life', 'away', 'world'), ('dreaming', 'life', 'away', 'world', 'cold'), ('life', 'away', 'world', 'cold', 'sane'), ('away', 'world', 'cold', 'sane', 'wheres'), ('world', 'cold', 'sane', 'wheres', 'shame'), ('cold', 'sane', 'wheres', 'shame', 'moment'), ('sane', 'wheres', 'shame', 'moment', 'time'), ('wheres', 'shame', 'moment', 'time', 'passes'), ('shame', 'moment', 'time', 'passes', 'can'), ('moment', 'time', 'passes', 'can', 'not'), ('time', 'passes', 'can', 'not', 'rewind'), ('passes', 'can', 'not', 'rewind', 'whos'), ('can', 'not', 'rewind', 'whos', 'blame'), ('not', 'rewind', 'whos', 'blame', 'start'), ('rewind', 'whos', 'blame', 'start', 'cure'), ('whos', 'blame', 'start', 'cure', 'sickness'), ('blame', 'start', 'cure', 'sickness', 'heart'), ('start', 'cure', 'sickness', 'heart', 'dont'), ('cure', 'sickness', 'heart', 'dont', 'believe'), ('sickness', 'heart', 'dont', 'believe', 'men'), ('heart', 'dont', 'believe', 'men', 'born'), ('dont', 'believe', 'men', 'born', 'killers'), ('believe', 'men', 'born', 'killers', 'dont'), ('men', 'born', 'killers', 'dont', 'believe'), ('born', 'killers', 'dont', 'believe', 'world'), ('killers', 'dont', 'believe', 'world', 'cant'), ('dont', 'believe', 'world', 'cant', 'saved'), ('believe', 'world', 'cant', 'saved', 'get'), ('world', 'cant', 'saved', 'get', 'start'), ('cant', 'saved', 'get', 'start', 'innocent'), ('saved', 'get', 'start', 'innocent', 'child'), ('get', 'start', 'innocent', 'child', 'thorn'), ('start', 'innocent', 'child', 'thorn', 'heart'), ('innocent', 'child', 'thorn', 'heart', 'kind'), ('child', 'thorn', 'heart', 'kind', 'world'), ('thorn', 'heart', 'kind', 'world', 'live'), ('heart', 'kind', 'world', 'live', 'love'), ('kind', 'world', 'live', 'love', 'divided'), ('world', 'live', 'love', 'divided', 'hate'), ('live', 'love', 'divided', 'hate', 'selling'), ('love', 'divided', 'hate', 'selling', 'soul'), ('divided', 'hate', 'selling', 'soul', 'reason'), ('hate', 'selling', 'soul', 'reason', 'must'), ('selling', 'soul', 'reason', 'must', 'dreaming'), ('soul', 'reason', 'must', 'dreaming', 'life'), ('reason', 'must', 'dreaming', 'life', 'away'), ('must', 'dreaming', 'life', 'away', 'world'), ('dreaming', 'life', 'away', 'world', 'cold'), ('life', 'away', 'world', 'cold', 'world'), ('away', 'world', 'cold', 'world', 'cold'), ('world', 'cold', 'world', 'cold', 'theres'), ('cold', 'world', 'cold', 'theres', 'sickness'), ('world', 'cold', 'theres', 'sickness', 'inside'), ('cold', 'theres', 'sickness', 'inside', 'wants'), ('theres', 'sickness', 'inside', 'wants', 'escape'), ('sickness', 'inside', 'wants', 'escape', 'feeling'), ('inside', 'wants', 'escape', 'feeling', 'get'), ('wants', 'escape', 'feeling', 'get', 'cant'), ('escape', 'feeling', 'get', 'cant', 'find'), ('feeling', 'get', 'cant', 'find', 'way'), ('get', 'cant', 'find', 'way', 'many'), ('cant', 'find', 'way', 'many', 'times'), ('find', 'way', 'many', 'times', 'must'), ('way', 'many', 'times', 'must', 'fall'), ('many', 'times', 'must', 'fall', 'knees'), ('times', 'must', 'fall', 'knees', 'never'), ('must', 'fall', 'knees', 'never', 'never'), ('fall', 'knees', 'never', 'never', 'never'), ('knees', 'never', 'never', 'never', 'never'), ('never', 'never', 'never', 'never', 'never'), ('never', 'never', 'never', 'never', 'starts'), ('never', 'never', 'never', 'starts', 'pain'), ('never', 'never', 'starts', 'pain', 'followed'), ('never', 'starts', 'pain', 'followed', 'hate'), ('starts', 'pain', 'followed', 'hate', 'dont'), ('pain', 'followed', 'hate', 'dont', 'believe'), ('followed', 'hate', 'dont', 'believe', 'men'), ('hate', 'dont', 'believe', 'men', 'born'), ('dont', 'believe', 'men', 'born', 'killers'), ('believe', 'men', 'born', 'killers', 'dont'), ('men', 'born', 'killers', 'dont', 'believe'), ('born', 'killers', 'dont', 'believe', 'world'), ('killers', 'dont', 'believe', 'world', 'cant'), ('dont', 'believe', 'world', 'cant', 'saved'), ('believe', 'world', 'cant', 'saved', 'kind'), ('world', 'cant', 'saved', 'kind', 'world'), ('cant', 'saved', 'kind', 'world', 'live'), ('saved', 'kind', 'world', 'live', 'love'), ('kind', 'world', 'live', 'love', 'divided'), ('world', 'live', 'love', 'divided', 'hate'), ('live', 'love', 'divided', 'hate', 'losing'), ('love', 'divided', 'hate', 'losing', 'control'), ('divided', 'hate', 'losing', 'control', 'feeling'), ('hate', 'losing', 'control', 'feeling', 'dreaming'), ('losing', 'control', 'feeling', 'dreaming', 'life'), ('control', 'feeling', 'dreaming', 'life', 'away'), ('feeling', 'dreaming', 'life', 'away', 'kind'), ('dreaming', 'life', 'away', 'kind', 'world'), ('life', 'away', 'kind', 'world', 'live'), ('away', 'kind', 'world', 'live', 'love'), ('kind', 'world', 'live', 'love', 'divided'), ('world', 'live', 'love', 'divided', 'hate'), ('live', 'love', 'divided', 'hate', 'selling'), ('love', 'divided', 'hate', 'selling', 'soul'), ('divided', 'hate', 'selling', 'soul', 'reason'), ('hate', 'selling', 'soul', 'reason', 'must'), ('selling', 'soul', 'reason', 'must', 'dreaming'), ('soul', 'reason', 'must', 'dreaming', 'life'), ('reason', 'must', 'dreaming', 'life', 'away'), ('must', 'dreaming', 'life', 'away', 'world'), ('dreaming', 'life', 'away', 'world', 'cold'), ('life', 'away', 'world', 'cold', 'world'), ('away', 'world', 'cold', 'world', 'cold')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(texts[0], n=1)))\n",
    "print(list(ngrams(texts[0], n=2)))\n",
    "print(list(ngrams(texts[0], n=3)))\n",
    "print(list(ngrams(texts[0], n=4)))\n",
    "print(list(ngrams(texts[0], n=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hvAHcRsoYhC"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(4, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkc_zoQvqful"
   },
   "outputs": [],
   "source": [
    "model = MLE(4) \n",
    "model.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiD9IV6or8jr",
    "outputId": "6d4c4027-f5fe-4e9d-c361-6e20a65a0926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28853\n",
      "17333\n",
      "968\n"
     ]
    }
   ],
   "source": [
    "print(model.counts['good'])\n",
    "print(model.counts['new'])\n",
    "print(model.counts['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDUZ0jk1sEDB",
    "outputId": "13054354-de31-4b7c-98c8-db9bb50bea86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002491084614252271\n",
      "0.001496481115268243\n",
      "0.0001295920010394988\n"
     ]
    }
   ],
   "source": [
    "print(model.score('good'))\n",
    "print(model.score('new'))\n",
    "print(model.score('question'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivCDirsksYqE",
    "outputId": "d48b3bd2-66cd-41e7-8992-498c9a203bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(model.counts[['seas']]['pirate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kxm-rQyGsqJK",
    "outputId": "cc816599-6b55-41bd-f7ae-252ade327b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ive thinking babe things aint went friends lovers arguing like fight try compromise hard let go wait around youll lose wife play long youll lose life got ta good life could go far high class woman tryn give line left early felt fine cant stay theres something wrong ironic aint thats way life thats thank god girls thank god girls halle jesu christe tennessee la thank god girls reckoning day better bow pray shes big shes strong shes energetic sweaty overalls thank god girls reckoning day better bow pray shes big shes strong shes soft shes honest really helps good\n"
     ]
    }
   ],
   "source": [
    "word_list = model.generate(100, random_seed=40)\n",
    "print(' '.join(word for word in word_list))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CA_Testing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
