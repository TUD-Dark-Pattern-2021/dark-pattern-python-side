{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Web Scraper\n",
    "\n",
    "### Use `selenium` to grab the data\n",
    "The first step is to scrape the TEXT data from the website, here we use `selenium` to be the automatic web driver to grab the data.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 95.0.4638\n",
      "Get LATEST driver version for 95.0.4638\n",
      "Driver [/Users/zenglan/.wdm/drivers/chromedriver/mac64/95.0.4638.54/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# web scraper\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import lxml.html\n",
    "import re\n",
    "import time\n",
    "# write to csv file\n",
    "import csv\n",
    "# joblib is a set of tools to provide lightweight pipelining in Python. It provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently.\n",
    "import joblib\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "url = 'https://eur.shein.com/category/Shoes-Bags-Accs-sc-00828516.html?ici=eur_tab01navbar09&scici=navbar_WomenHomePage~~tab01navbar09~~9~~webLink~~~~0&srctype=category&userpath=category%3ESHOES-ACCESSORIES'\n",
    "\n",
    "# to avoid opening browser while using selenium\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options=option)\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# get source code -- type: str\n",
    "html_source = driver.page_source\n",
    "\n",
    "# key\n",
    "html = lxml.html.fromstring(html_source)\n",
    "\n",
    "# obtain all the text under the 'div' tags\n",
    "items = html.xpath(\"//div//text()\")\n",
    "\n",
    "pattern = re.compile(\"^\\s+|\\s+$|\\n\")\n",
    "\n",
    "clause_text = \"\"\n",
    "\n",
    "for item in items:\n",
    "    line = re.sub(pattern, \"\", item)\n",
    "    if len(item) > 1:\n",
    "        clause_text += line +\"\\n\"\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "#  Initial Filtering and Generate CSV\n",
    "# (1) Only keep the content have the number of words ranging from 2 to 20.\n",
    "# (2) Ignore the content beginning with the characters in the ignore string list, such as \"{\", \".\", \";\" and so on.\n",
    "# (3) Save the filtered content into one column of the created csv file.\n",
    "\n",
    "\n",
    "raw_text = clause_text\n",
    "\n",
    "# the beginning character of the content, which is the sign we should ignore the content\n",
    "ignore_str = ',.;{}#?/!()@$'\n",
    "\n",
    "# the content we are going to keep to send to models.\n",
    "content_list = []\n",
    "\n",
    "# only keep the content that has words count from 2 to 50 (includes).\n",
    "for line in raw_text.split('\\n'):\n",
    "    if 1<len(line.split())<=20 and line[0] not in ignore_str:\n",
    "        content_list.append([line])\n",
    "\n",
    "\n",
    "header = ['content']\n",
    "\n",
    "# create a csv file to save the filtered content for later model analysis.\n",
    "with open('Raw/shein-4.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write the data\n",
    "    writer.writerows(content_list)\n",
    "        \n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# Step 2: Checking Presence\n",
    "\n",
    "## Use pretrained presence model to check the presence in the filtered content\n",
    "#(1) Load the pretrained model and countvectorizer.\n",
    "#(2) Transform the content using the countvectorizer.\n",
    "#(3) Use the model for presence prediction on the content.\n",
    "\n",
    "# Loading the saved model with joblib\n",
    "presence_model = joblib.load('rf_presence_classifier.joblib')\n",
    "presence_cv = joblib.load('presence_CountVectorizer.joblib')\n",
    "\n",
    "# New dataset to predict\n",
    "presence_pred = pd.read_csv('Raw/shein-4.csv')\n",
    "\n",
    "# apply the pretrained model to the new content data\n",
    "pre_pred_vec = presence_model.predict(presence_cv.transform(presence_pred['content']))\n",
    "\n",
    "presence_pred['presence'] = pre_pred_vec.tolist()\n",
    "\n",
    "# dark pattern content are those where the predicted result equals to 0.\n",
    "dark = presence_pred.loc[presence_pred['presence']==0]\n",
    "\n",
    "dark.to_csv('DP/shein-4.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
