{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-7-10497e839cea>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-10497e839cea>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    url = 'https://www.currys.ie/ieen/tv-and-home-entertainment/televisions/televisions/301_3002_30002_xx_ba00013738-bv00313852%7Cbv00313851/xx-criteria.html\u001b[0m\n\u001b[0m                                                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# web scraper\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import lxml.html\n",
    "import re\n",
    "import time\n",
    "\n",
    "# write to csv file\n",
    "\n",
    "import csv\n",
    "\n",
    "# joblib is a set of tools to provide lightweight pipelining in Python. It provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently.\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------- Web Scrapper ---------------------------\n",
    "\n",
    "url = 'https://www.currys.ie/ieen/tv-and-home-entertainment/televisions/televisions/301_3002_30002_xx_ba00013738-bv00313852%7Cbv00313851/xx-criteria.html\n",
    "'\n",
    "#url='https://www.currys.ie/ieen/search-keywords/xx_xx_xx_xx_xx/-wk22_headphones_ie-/xx-criteria.html'\n",
    "\n",
    "# to avoid opening browser while using selenium\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options=option)\n",
    "\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# get source code -- type: str\n",
    "html_source = driver.page_source\n",
    "\n",
    "# key\n",
    "html = lxml.html.fromstring(html_source)\n",
    "\n",
    "# obtain all the text under the 'div' tags\n",
    "items = html.xpath(\"//text()\")\n",
    "\n",
    "pattern = re.compile(\"^\\s+|\\s+$|\\n\")\n",
    "\n",
    "clause_text = \"\"\n",
    "\n",
    "for item in items:\n",
    "    line = re.sub(pattern, \"\", item)\n",
    "    if len(item) > 1:\n",
    "        clause_text += line +\"\\n\"\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------- Scrapping Dataset ---------------------------\n",
    "\n",
    "raw_text = clause_text\n",
    "\n",
    "# the beginning character of the content, which is the sign we should ignore the content\n",
    "ignore_str = ',.;{}?#/)!('\n",
    "\n",
    "# the content we are going to keep to send to models.\n",
    "content_list = []\n",
    "\n",
    "# only keep the content that has words count from 2 to 20 (includes).\n",
    "for line in raw_text.split('\\n'):\n",
    "    if 1<len(line.split())<21 and line[0] not in ignore_str:\n",
    "        content_list.append([line])\n",
    "\n",
    "header = ['content']\n",
    "\n",
    "# create a csv file to save the filtered content for later model analysis.\n",
    "with open('Websites/pc01.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write the data\n",
    "    writer.writerows(content_list)\n",
    "    \n",
    "    \n",
    "        \n",
    "# -------------------------------- Check Presence ---------------------------    \n",
    "\n",
    "\n",
    "# Loading the saved model with joblib\n",
    "presence_model = joblib.load('bnb_presence_classifier.joblib')\n",
    "presence_cv = joblib.load('presence_TfidfVectorizer.joblib')\n",
    "\n",
    "# New dataset to predict\n",
    "presence_pred = pd.read_csv('Websites/pc01.csv')\n",
    "\n",
    "\n",
    "# Filter out the disturibing content to be removed\n",
    "str_list = ['low to high','high to low','high low','low high','{','ships','ship','Â®','details',\n",
    "            'limited edition','cart is currently empty','in cart','out of stock','believe in',\n",
    "            'today\\'s deals','customer service','offer available','offers available', 'collect',\n",
    "            '% off','in stock soon','problem','UTC','javascript','cookie','cookies','disclaimer']\n",
    "pattern = '|'.join(str_list)\n",
    "\n",
    "presence_pred = presence_pred[~presence_pred.content.str.lower().str.contains(pattern)]\n",
    "\n",
    "\n",
    "\n",
    "# apply the pretrained model to the new content data\n",
    "pre_pred_vec = presence_model.predict(presence_cv.transform(presence_pred['content']))\n",
    "\n",
    "presence_pred['presence'] = pre_pred_vec.tolist()\n",
    "\n",
    "# dark pattern content are those where the predicted result equals to 0.\n",
    "dark = presence_pred.loc[presence_pred['presence']==0]\n",
    "\n",
    "dark.to_csv('Websites/pc01-bnb.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lr_category_classifier.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-df9242eae6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the saved model with joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr_category_classifier.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcat_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type_CountVectorizer.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# mapping of the encoded dark pattern categories.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lr_category_classifier.joblib'"
     ]
    }
   ],
   "source": [
    "# Loading the saved model with joblib\n",
    "cat_model = joblib.load('lr_category_classifier.joblib')\n",
    "cat_cv = joblib.load('type_CountVectorizer.joblib')\n",
    "\n",
    "# mapping of the encoded dark pattern categories.\n",
    "cat_dic = {0:'Activity Notification', 1:'Countdown Timer', 2:'High-demand Message', \n",
    "           3:'Limited-time Message', 4:'Low-stock Message'}\n",
    "\n",
    "# apply the model and the countvectorizer to the detected dark pattern content data\n",
    "cat_pred_vec = cat_model.predict(cat_cv.transform(dark['content']))\n",
    "\n",
    "\n",
    "dark['type'] = cat_pred_vec.tolist()\n",
    "\n",
    "type_list = dark['type'].tolist()\n",
    "\n",
    "# get the mapping of the category name and encoded category integers\n",
    "dark['type_name'] = [cat_dic[int(type)] for type in type_list]\n",
    "\n",
    "# reset the index of the detected dark pattern list on the webpage.\n",
    "dark = dark.reset_index(drop=True)\n",
    "\n",
    "dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
